```python
# 这里有疑问：为什么在d_model中，偶数元素使用sin函数，奇数元素使用cos函数。
# 为什么就不能全部使用sin函数呢？或者cos函数呢？这有什么原理吗？
# 能否通过可视化代码方式给我讲明白呢？
# 这里我还无法感知到，位置信息嵌入了吗？每个位置的表示是不是都不相同？
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.) / d_model))
        pe[:, 0::2] = torch.sin(position*div_term) # 广播机制，使得其维度变成(max_len, d_model/2)
        pe[:, 1::2] = torch.cos(position*div_term)
        pe = pe.unsqueeze(0) # [1, max_len, d_model]
        self.register_buffer("pe", pe)
    
    def forward(self, x):
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :]
        
        return x
```

---

![alt text](image-43.png)

![alt text](image-44.png)

![alt text](image-45.png)

![alt text](image-46.png)

![alt text](image-47.png)

![alt text](image-48.png)

![alt text](image-49.png)

![alt text](image-50.png)

**可视化信息：**

![alt text](image-51.png)

![alt text](image-52.png)

**从结果来看：**

随着i的增大，pos/10000,如果pos较小。d_model靠后面的元素越来越小。即sin（0） ， cos（0）。越来越趋近于一个常数。我想表达的意思是。每个元素的后面的值都去向于这样的情况，特别是前期的一些position。且随着，分母的增大。相邻元素的变化其实不大。既然位置信息，不应该区分d_model中每个元素都有位置差的关系吗？从这个角度来看，他并不是每个元素都有位置差的概念？虽然单个词具有了位置差的概念。但每个词的d_model元素是参差不齐的。这个应该还有优化空间吧？

![alt text](image-53.png)

![alt text](image-54.png)

![alt text](image-55.png)

![alt text](image-56.png)

![alt text](image-57.png)

![alt text](image-58.png)

![alt text](image-59.png)

![alt text](image-60.png)

![alt text](image-61.png)

![alt text](image-62.png)


**可视化信息：**

![alt text](image-63.png)
从第一张图可以看出，维度越靠后面，变化越小。位置嵌入越不明显

![alt text](image-64.png)
相邻位置元素差异。相邻位置的词，只有d_model靠前e的元素影响较大。到后面差异几乎为0

![alt text](image-65.png)
从这张图，可以明显看出来，相邻位置的词，位置差异的影响，只有在前面几个元素。

**这种设计是不合理的！**
我觉得，既然是位置编码，就不要用d_model前面元素和位置相关性高一些，中间低一些，越后面位置相关性越小。这就是不合理的。我打个比方，我和你距离1米，难道我的头距离1米，我手脚可以距离0.5米，我的屁股可以和你挨着吗？这显然是不对的想法。既然你的职责就是表位置。那么我们d_model中每个元素都应该感受出距离。而不是你用这种方式来解释。我觉得不恰当。所以应该是更多的考虑，让每个元素都贴上一个相同距离的标签，这个距离可以是角度距离，可以是欧拉距离等等。

![alt text](image-66.png)

![alt text](image-67.png)

![alt text](image-68.png)

![alt text](image-69.png)

```python
import torch
import math
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

class UniformPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, base_freq=0.1):
        super().__init__()
        assert d_model % 2 == 0, "d_model must be even for sin/cos pairs"
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]
        
        # 每对维度 (sin, cos) 对应一个频率
        num_pairs = d_model // 2
        # 频率线性变化，从 base_freq 到 base_freq * 2
        freqs = torch.linspace(base_freq, base_freq * 2, num_pairs)  # [d_model/2]
        freqs = freqs.unsqueeze(0)  # [1, d_model/2]
        
        # 计算角度
        angles = position * freqs  # [max_len, d_model/2]
        
        # 偶数维度用 sin，奇数维度用 cos
        pe[:, 0::2] = torch.sin(angles)
        pe[:, 1::2] = torch.cos(angles)
        
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer("pe", pe)
    
    def forward(self, x):
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :]
        return x

# 参数
d_model = 128
max_len = 50
base_freq = 0.1  # 基础频率

# 初始化位置编码
pe_module = UniformPositionalEncoding(d_model, max_len, base_freq)
pe = pe_module.pe.squeeze(0).numpy()  # [max_len, d_model] -> [50, 128]

# 可视化 1：位置编码热图
plt.figure(figsize=(15, 5))
sns.heatmap(pe, cmap='RdBu', vmin=-1, vmax=1)
plt.title("Uniform Positional Encoding (d_model=128, max_len=50)")
plt.xlabel("Dimension (d_model)")
plt.ylabel("Position (pos)")
plt.show()

# 可视化 2：不同维度的值随位置变化
dims = [0, 1, 20, 21, 60, 61, 126, 127]
plt.figure(figsize=(10, 5))
for dim in dims:
    plt.plot(pe[:, dim], label=f"Dim {dim}")
plt.title("Uniform Positional Encoding Values Across Positions")
plt.xlabel("Position (pos)")
plt.ylabel("Value")
plt.legend()
plt.show()

# 可视化 3：相邻位置的差异
positions = [0, 1, 2, 3, 4]
diffs = []
for pos in range(len(positions)-1):
    diff = pe[positions[pos+1]] - pe[positions[pos]]
    diffs.append(diff)

plt.figure(figsize=(10, 5))
for i, diff in enumerate(diffs):
    plt.plot(diff, label=f"Diff between Pos {positions[i]} and {positions[i+1]}")
plt.title("Difference Between Adjacent Positions (Uniform PE)")
plt.xlabel("Dimension (d_model)")
plt.ylabel("Difference")
plt.legend()
plt.show()

# 可视化 4：不同维度的差异分布
diffs_all = np.abs(pe[1:] - pe[:-1])  # [49, 128]
plt.figure(figsize=(15, 5))
sns.heatmap(diffs_all, cmap='Blues', vmin=0, vmax=0.5)
plt.title("Absolute Difference Between Adjacent Positions (Uniform PE)")
plt.xlabel("Dimension (d_model)")
plt.ylabel("Position Pair (pos to pos+1)")
plt.show()
```

![alt text](image-70.png)

![alt text](image-71.png)

![alt text](image-72.png)

![alt text](image-73.png)

![alt text](image-74.png)

![alt text](image-75.png)

```python
class EuclideanPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, scale=1.0):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]
        
        # 每个维度对应一个方向，位置 pos 在该方向上移动
        direction = torch.randn(d_model)  # 随机方向
        direction = direction / torch.norm(direction)  # 归一化到单位向量
        pe = position * direction * scale  # 每个位置沿方向移动 scale 距离
        
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer("pe", pe)
    
    def forward(self, x):
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :]
        return x

# 参数
d_model = 128
max_len = 50
scale = 1.0

# 初始化位置编码
pe_module = EuclideanPositionalEncoding(d_model, max_len, scale)
pe = pe_module.pe.squeeze(0).numpy()  # [50, 128]

# 验证欧氏距离
dists = np.linalg.norm(pe[1:] - pe[:-1], axis=1)
print("Euclidean distances between adjacent positions:", dists[:5])

# 可视化：不同维度的值随位置变化
dims = [0, 1, 20, 21, 60, 61, 126, 127]
plt.figure(figsize=(10, 5))
for dim in dims:
    plt.plot(pe[:, dim], label=f"Dim {dim}")
plt.title("Euclidean Positional Encoding Values Across Positions")
plt.xlabel("Position (pos)")
plt.ylabel("Value")
plt.legend()
plt.show()
```

欧式距离还是很难设计出较好的位置编码的
通过呈现效果也能知道，这个肯定不行的，数值的递增，位置编码的权重过大，掩盖了参数的其他信息

![alt text](image-76.png)

![alt text](image-77.png)

![alt text](image-78.png)