{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1000000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\", \"en-zh\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Sixty-first session', 'zh': '第六十一届会议'}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "class T5Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "class PositionalEncoding(nn.Module): \n",
    "    def __init__(self, d_model, max_len=5000): \n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len].to(x.device)\n",
    "\n",
    "\n",
    "pe = PositionalEncoding(512)\n",
    "x = torch.randn(size=(2,5,512))\n",
    "# print(x[1,0,:])\n",
    "\n",
    "[para.numel() for para in pe.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model必须被num_heads整除\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)    \n",
    "        self.k_linear = nn.Linear(d_model, d_model)    \n",
    "        self.v_linear = nn.Linear(d_model, d_model)    \n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None): \n",
    "        batch_size = q.size(0)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None: \n",
    "            scores = scores.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, v)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.out_linear(output)\n",
    "    \n",
    "\n",
    "attn = MultiHeadAttention(512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512]\n",
      "1050624\n"
     ]
    }
   ],
   "source": [
    "params = [param.numel() for param in attn.parameters()]\n",
    "print(params)\n",
    "sum = 0\n",
    "for i in params:\n",
    "    sum += i \n",
    "print(sum)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1048576, 2048, 1048576, 512]\n",
      "2099712\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.linear2(torch.relu(self.linear1(x)))\n",
    "    \n",
    "ffn = FeedForward(512, 2048)\n",
    "param = [para.numel() for para in ffn.parameters()]\n",
    "print(param)\n",
    "sum = 0 \n",
    "for i in param:\n",
    "    sum += i\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512, 1048576, 2048, 1048576, 512, 512, 512, 512, 512]\n",
      "3152384\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x \n",
    "\n",
    "encoder_block = EncoderLayer(512, 8, 2048, 0.1)\n",
    "params = [param.numel() for param in encoder_block.parameters()]\n",
    "print(params)\n",
    "sum = 0 \n",
    "for i in params:\n",
    "    sum += i\n",
    "    \n",
    "# 参数量统计\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 1048576, 2048, 1048576, 512, 512, 512, 512, 512, 512, 512]\n",
      "4204032\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model) \n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None): \n",
    "         x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), tgt_mask))\n",
    "         x = x + self.dropout(self.cross_attn(self.norm2(x), enc_output, enc_output, src_mask))\n",
    "         x = x + self.dropout(self.ff(self.norm3(x)))\n",
    "         return x \n",
    "     \n",
    "decoder_block = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "params = [para.numel() for para in decoder_block.parameters()]\n",
    "print(params)\n",
    "sum = 0\n",
    "for i in params:\n",
    "    sum += i\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.embedding.weight ==> 参数量：15360000\n",
      "encoder_layers.0.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.0.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.0.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.0.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.0.norm1.weight ==> 参数量：768\n",
      "encoder_layers.0.norm1.bias ==> 参数量：768\n",
      "encoder_layers.0.norm2.weight ==> 参数量：768\n",
      "encoder_layers.0.norm2.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.1.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.1.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.1.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.1.norm1.weight ==> 参数量：768\n",
      "encoder_layers.1.norm1.bias ==> 参数量：768\n",
      "encoder_layers.1.norm2.weight ==> 参数量：768\n",
      "encoder_layers.1.norm2.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.2.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.2.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.2.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.2.norm1.weight ==> 参数量：768\n",
      "encoder_layers.2.norm1.bias ==> 参数量：768\n",
      "encoder_layers.2.norm2.weight ==> 参数量：768\n",
      "encoder_layers.2.norm2.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.3.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.3.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.3.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.3.norm1.weight ==> 参数量：768\n",
      "encoder_layers.3.norm1.bias ==> 参数量：768\n",
      "encoder_layers.3.norm2.weight ==> 参数量：768\n",
      "encoder_layers.3.norm2.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.4.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.4.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.4.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.4.norm1.weight ==> 参数量：768\n",
      "encoder_layers.4.norm1.bias ==> 参数量：768\n",
      "encoder_layers.4.norm2.weight ==> 参数量：768\n",
      "encoder_layers.4.norm2.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.5.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.5.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.5.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.5.norm1.weight ==> 参数量：768\n",
      "encoder_layers.5.norm1.bias ==> 参数量：768\n",
      "encoder_layers.5.norm2.weight ==> 参数量：768\n",
      "encoder_layers.5.norm2.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.0.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.0.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.0.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.0.norm1.weight ==> 参数量：768\n",
      "decoder_layers.0.norm1.bias ==> 参数量：768\n",
      "decoder_layers.0.norm2.weight ==> 参数量：768\n",
      "decoder_layers.0.norm2.bias ==> 参数量：768\n",
      "decoder_layers.0.norm3.weight ==> 参数量：768\n",
      "decoder_layers.0.norm3.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.1.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.1.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.1.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.1.norm1.weight ==> 参数量：768\n",
      "decoder_layers.1.norm1.bias ==> 参数量：768\n",
      "decoder_layers.1.norm2.weight ==> 参数量：768\n",
      "decoder_layers.1.norm2.bias ==> 参数量：768\n",
      "decoder_layers.1.norm3.weight ==> 参数量：768\n",
      "decoder_layers.1.norm3.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.2.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.2.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.2.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.2.norm1.weight ==> 参数量：768\n",
      "decoder_layers.2.norm1.bias ==> 参数量：768\n",
      "decoder_layers.2.norm2.weight ==> 参数量：768\n",
      "decoder_layers.2.norm2.bias ==> 参数量：768\n",
      "decoder_layers.2.norm3.weight ==> 参数量：768\n",
      "decoder_layers.2.norm3.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.3.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.3.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.3.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.3.norm1.weight ==> 参数量：768\n",
      "decoder_layers.3.norm1.bias ==> 参数量：768\n",
      "decoder_layers.3.norm2.weight ==> 参数量：768\n",
      "decoder_layers.3.norm2.bias ==> 参数量：768\n",
      "decoder_layers.3.norm3.weight ==> 参数量：768\n",
      "decoder_layers.3.norm3.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.4.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.4.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.4.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.4.norm1.weight ==> 参数量：768\n",
      "decoder_layers.4.norm1.bias ==> 参数量：768\n",
      "decoder_layers.4.norm2.weight ==> 参数量：768\n",
      "decoder_layers.4.norm2.bias ==> 参数量：768\n",
      "decoder_layers.4.norm3.weight ==> 参数量：768\n",
      "decoder_layers.4.norm3.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.5.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.5.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.5.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.5.norm1.weight ==> 参数量：768\n",
      "decoder_layers.5.norm1.bias ==> 参数量：768\n",
      "decoder_layers.5.norm2.weight ==> 参数量：768\n",
      "decoder_layers.5.norm2.bias ==> 参数量：768\n",
      "decoder_layers.5.norm3.weight ==> 参数量：768\n",
      "decoder_layers.5.norm3.bias ==> 参数量：768\n",
      "output_layer.weight ==> 参数量：15360000\n",
      "output_layer.bias ==> 参数量：20000\n",
      "111091232\n"
     ]
    }
   ],
   "source": [
    "# 组合T5模型\n",
    "class T5Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = T5Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src_input, tgt_input, src_mask=None, tgt_mask=None): \n",
    "        src_emb = self.pos_encoding(self.embedding(src_input))\n",
    "        tgt_emb = self.pos_encoding(self.embedding(tgt_input))\n",
    "        \n",
    "        enc_output = src_emb \n",
    "        for layer in self.encoder_layers:\n",
    "            enc_output = layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return self.output_layer(dec_output)\n",
    "    \n",
    "t5 = T5Model(20000, 768, 8, 2048, 6, 0.1)\n",
    "\n",
    "sum = 0 \n",
    "\n",
    "for name,param in t5.named_parameters(): \n",
    "    print(f\"{name} ==> 参数量：{param.numel()}\")\n",
    "    sum += param.numel()\n",
    "    \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  构建词汇表 (标点符号是不是也在内了)\n",
    "from collections import Counter\n",
    "\n",
    "train_en = [item[\"translation\"][\"en\"] for item in dataset[\"train\"]]\n",
    "train_zh = [item[\"translation\"][\"zh\"] for item in dataset[\"train\"]]\n",
    "\n",
    "en_words = [word.lower() for sentence in train_en for word in sentence.split()]\n",
    "zh_chars = [char for sentence in train_zh for char in sentence]\n",
    "\n",
    "en_counter = Counter(en_words)\n",
    "zh_counter = Counter(zh_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16702317\n",
      "32233389\n",
      "16702317\n",
      "32233389\n"
     ]
    }
   ],
   "source": [
    "print(len(en_words))\n",
    "print(len(zh_chars))\n",
    "print(en_counter.total())\n",
    "print(zh_counter.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70658\n",
      "6334\n"
     ]
    }
   ],
   "source": [
    "min_freq = 5\n",
    "en_vocab = [word for word, freq in en_counter.items() if freq >= min_freq]\n",
    "zh_vocab = [char for char, freq in zh_counter.items() if freq >= min_freq]\n",
    "print(len(en_vocab))\n",
    "print(len(zh_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76901\n",
      "76905\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(en_vocab + zh_vocab))\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "vocab = ['<PAD>','<UNK>','<BOS>','<EOS>'] + vocab\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小：76905\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "\n",
    "print(f\"词汇表大小：{len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本编码\n",
    "def encode_text(text, word_to_id, is_zh = False): \n",
    "    if is_zh:\n",
    "        tokens = list(text)\n",
    "    else: \n",
    "        tokens = text.lower().split()\n",
    "    token_ids = [word_to_id.get(token, word_to_id['<UNK>']) for token in tokens]\n",
    "    return [word_to_id['<BOS>']] + token_ids + [word_to_id['<EOS>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset): \n",
    "    def __init__(self, dataset, word_to_id, max_length = 10):\n",
    "        self.dataset = dataset\n",
    "        self.word_to_id = word_to_id\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        translation = self.dataset[index]['translation']\n",
    "        src_ids = encode_text(translation[\"en\"], self.word_to_id, is_zh=False)\n",
    "        tgt_ids = encode_text(translation[\"zh\"], self.word_to_id, is_zh=True)\n",
    "        \n",
    "        src_ids = src_ids[:self.max_length] + [self.word_to_id['<PAD>']]*(self.max_length - len(src_ids))\n",
    "        tgt_input = tgt_ids[:-1][:self.max_length] + [self.word_to_id['<PAD>']]*(self.max_length - len(tgt_ids[:-1]))\n",
    "        tgt_output = tgt_ids[1:][:self.max_length] + [self.word_to_id['<PAD>']]*(self.max_length - len(tgt_ids[1:]))\n",
    "        \n",
    "        return {\n",
    "            'src_input': torch.tensor(src_ids, dtype=torch.long),\n",
    "            'tgt_input': torch.tensor(tgt_input, dtype=torch.long),\n",
    "            'tgt_output':torch.tensor(tgt_output, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "train_dataset = TranslationDataset(dataset=dataset[\"train\"], word_to_id=word_to_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7c69b676b550>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edwin/miniconda3/envs/pytorch/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "\n",
    "model = T5Model(vocab_size, d_model, num_heads, d_ff, num_layers, dropout)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=word_to_id['<PAD>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122966121\n"
     ]
    }
   ],
   "source": [
    "param_num = [para.numel() for para in model.parameters()]\n",
    "sum = 0 \n",
    "for n in param_num:\n",
    "    sum += n\n",
    "    \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt, pad_idx): \n",
    "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_seq_len = tgt.size(1)\n",
    "    tgt_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len))).bool().to(tgt.device)\n",
    "    tgt_mask = tgt_mask & (tgt != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return src_mask, tgt_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 10, 10])\n",
      "Epoch 1 / 1, Loss:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 1 \n",
    "for epoch in range(num_epochs): \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        src_input = batch['src_input'].to(device)\n",
    "        tgt_input = batch['tgt_input'].to(device)\n",
    "        tgt_output = batch['tgt_output'].to(device)\n",
    "        \n",
    "        src_mask, tgt_mask = create_mask(src_input, tgt_input, word_to_id['<PAD>'])\n",
    "        print(tgt_mask.shape)\n",
    "        \n",
    "        break\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs}, Loss:{total_loss / len(train_loader)}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/10417 [00:05<1:06:30,  2.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m src_mask, tgt_mask \u001b[38;5;241m=\u001b[39m create_mask(src_input, tgt_input, word_to_id[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), tgt_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36mT5Model.forward\u001b[0;34m(self, src_input, tgt_input, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m tgt_emb\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[0;32m---> 21\u001b[0m     dec_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(dec_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, enc_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, enc_output, src_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tgt_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m): \n\u001b[1;32m     13\u001b[0m      x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), tgt_mask))\n\u001b[0;32m---> 14\u001b[0m      x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, enc_output, enc_output, src_mask))\n\u001b[1;32m     15\u001b[0m      x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x)))\n\u001b[1;32m     16\u001b[0m      \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2892\u001b[0m         layer_norm,\n\u001b[1;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2899\u001b[0m     )\n\u001b[0;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10 \n",
    "for epoch in range(num_epochs): \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        src_input = batch['src_input'].to(device)\n",
    "        tgt_input = batch['tgt_input'].to(device)\n",
    "        tgt_output = batch['tgt_output'].to(device)\n",
    "        \n",
    "        src_mask, tgt_mask = create_mask(src_input, tgt_input, word_to_id['<PAD>'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src_input, tgt_input, src_mask, tgt_mask)\n",
    "        loss = loss_fn(logits.view(-1, vocab_size), tgt_output.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs}, Loss:{total_loss / len(train_loader)}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译结果：这是个疯狂的\n"
     ]
    }
   ],
   "source": [
    "def translate(model, src_text, word_to_id, id_to_word, max_length=50): \n",
    "    model.eval()\n",
    "    src_ids = encode_text(src_text, word_to_id, is_zh=False)\n",
    "    src_ids = src_ids[:max_length] + [word_to_id['<PAD>']]*(max_length - len(src_ids))\n",
    "    src_input = torch.tensor([src_ids], dtype=torch.long).to(device)\n",
    "    src_mask = (src_input != word_to_id['<PAD>']).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    tgt_ids = [word_to_id['<BOS>']]\n",
    "    for _ in range(max_length): \n",
    "        tgt_input = torch.tensor([tgt_ids], dtype=torch.long).to(device)\n",
    "        tgt_mask = torch.tril(torch.ones((len(tgt_ids), len(tgt_ids)))).bool().to(device)\n",
    "        tgt_mask = tgt_mask & (tgt_input != word_to_id['<PAD>']).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(src_input, tgt_input, src_mask, tgt_mask)\n",
    "            next_token = logits[0, -1].argmax().item()\n",
    "            \n",
    "        if next_token == word_to_id['<EOS>']:\n",
    "            break\n",
    "        \n",
    "        tgt_ids.append(next_token)\n",
    "    \n",
    "    return ''.join([id_to_word.get(idx, '<UNK>') for idx in tgt_ids[1:]])\n",
    "\n",
    "\n",
    "src_text = \"This is a test!\"\n",
    "\n",
    "translated = translate(model, src_text, word_to_id, id_to_word)\n",
    "\n",
    "print(f\"翻译结果：{translated}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
