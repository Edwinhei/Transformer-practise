{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标\n",
    "\n",
    "1. 使用 transformers 的 tokenizers 模块从头开始训练一个分词器。\n",
    "2. 将语料直接加载到内存中，避免文件 I/O。\n",
    "\n",
    "## 依赖项\n",
    "确保你已经安装了 transformers 和 tokenizers 库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码实现\n",
    "\n",
    "以下代码将：\n",
    "\n",
    "1. 加载语料到内存。\n",
    "2. 使用 tokenizers 模块训练一个 BPE 分词器，完全在内存中操作。\n",
    "3. 保存训练好的分词器，并测试分词效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载2000000行到内存，耗时0.42秒\n",
      "使用 2000000 行进行训练\n",
      "\n",
      "\n",
      "\n",
      "BPE 分词器训练完成，耗时 29.93 秒\n",
      "分词器已保存到my_tokenizer.json\n",
      "\n",
      "分词结果：\n",
      "混合文本: ['Hello', ',', 'world', '!', '你好', '，', '世界', '！']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "import time\n",
    "\n",
    "# 1. 加载语料到内存\n",
    "def load_corpus_to_memory(file_path):\n",
    "    start_time = time.time()\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f: \n",
    "        lines = f.readlines()\n",
    "    print(f\"已加载{len(lines)}行到内存，耗时{time.time() - start_time:.2f}秒\")\n",
    "    return lines\n",
    "\n",
    "# 2. 训练BPE分词器（完全在内存中）\n",
    "def train_bpe_tokenizer(corpus, vocab_size=8000, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"]): \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 初始化一个BPE分词器\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    \n",
    "    # 设置预分词器（按空格和标点分词）\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    # 定义BPE训练器\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        min_frequency=1,# 最小频率，低于此频率的 token 不会加入词汇表\n",
    "        show_progress=True # 显示训练进度\n",
    "    )\n",
    "    \n",
    "    # 直接在内存中训练\n",
    "    tokenizer.train_from_iterator(corpus, trainer)\n",
    "    \n",
    "    print(f\"BPE 分词器训练完成，耗时 {time.time() - start_time:.2f} 秒\")\n",
    "    return tokenizer\n",
    "\n",
    "# 3. 保存分词器\n",
    "def save_tokenizer(tokenizer, path=\"my_tokenizer\"): \n",
    "    tokenizer.save(path)\n",
    "    print(f\"分词器已保存到{path}\")\n",
    "    \n",
    "# 4. 分词测试\n",
    "def tokenize_text(tokenizer, text):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return encoded.tokens \n",
    "\n",
    "# 5. 主函数\n",
    "def main(): \n",
    "    # 加载语料到内存\n",
    "    corpus = load_corpus_to_memory(\"corpus.txt\")\n",
    "    \n",
    "    # 为了快速体验，只用前5万行\n",
    "    # corpus = corpus[:500000]\n",
    "    print(f\"使用 {len(corpus)} 行进行训练\")\n",
    "    \n",
    "    # 训练BPE分词器\n",
    "    tokenizer = train_bpe_tokenizer(corpus, vocab_size=32000)\n",
    "    \n",
    "    # 保存分词器\n",
    "    save_tokenizer(tokenizer, \"my_tokenizer.json\")\n",
    "    \n",
    "    # 测试分词效果\n",
    "    text = \"Hello, world!你好，世界！\"\n",
    "    tokens = tokenize_text(tokenizer, text)\n",
    "    print(\"\\n分词结果：\")   \n",
    "    print(f\"混合文本: {tokens}\")  \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 代码说明\n",
    "\n",
    "1. **加载语料到内存**\n",
    "- load_corpus_to_memory() 将 corpus.txt 读入内存，存储为一个字符串列表。\n",
    "- 500 MB 的文件加载到内存通常只需不到 1 秒。\n",
    "\n",
    "2. **训练 BPE 分词器**\n",
    "\n",
    "- 初始化分词器：Tokenizer(models.BPE()) 创建一个基于 BPE 算法的分词器。\n",
    "- 预分词器：pre_tokenizers.Whitespace() 按空格和标点进行预分词，确保英文按词拆分，中文按字符处理。\n",
    "- 训练器：BpeTrainer 定义了训练参数：\n",
    "  - `vocab_size=8000`：目标词汇表大小。\n",
    "  - `special_tokens`：添加特殊 token（如 [PAD]、[UNK] 等），适合后续与预训练模型结合。\n",
    "  - `min_frequency=2`：最低频率，减少低频 token。\n",
    "  - `show_progress=True`：显示训练进度。\n",
    "- 内存训练：train_from_iterator() 直接从内存中的 corpus（字符串列表）训练分词器，不需要写入文件。\n",
    "\n",
    "3. **保存分词器**\n",
    "- `save_tokenizer()` 将训练好的分词器保存为 JSON 文件（`my_tokenizer.json`），可以后续加载使用。\n",
    "\n",
    "4. **分词测试**\n",
    "- tokenize_text() 使用训练好的分词器对文本进行分词，返回 token 列表。\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 预期效果\n",
    "- **内存加载**：500 MB 文件加载到内存只需不到 1 秒。\n",
    "- **训练时间**：5 万行数据，vocab_size=8000，在 20 核心 CPU 上可能只需 1-2 分钟（tokenizers 底层是用 Rust 实现的，效率很高）。\n",
    "- **CPU 使用率**：tokenizers 会自动利用多线程，CPU 使用率应该较高（接近 100%）。\n",
    "- **分词结果**：训练完成后，你会得到一个分词器，可以处理中英文混合文本。\n",
    "\n",
    "**示例输出**\n",
    "\n",
    "```bash\n",
    "已加载 1000000 行到内存，耗时 0.85 秒\n",
    "使用 50000 行进行训练\n",
    "BPE 分词器训练完成，耗时 90.32 秒\n",
    "分词器已保存到 my_tokenizer.json\n",
    "\n",
    "分词结果：\n",
    "混合文本: ['Hello', ',', 'world', '!', '你好', '，', '世界', '！']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 与 SentencePiece 的对比\n",
    "\n",
    "- 内存训练：tokenizers 支持直接从内存中的字符串列表训练（train_from_iterator），完全避免了文件 I/O，而 SentencePiece 要求输入文件路径。\n",
    "- 速度：tokenizers 底层是用 Rust 实现的，训练速度通常比 SentencePiece 快，尤其是在多线程场景下。\n",
    "- 分词效果：两者都使用 BPE 算法，分词结果类似，但 tokenizers 提供了更多灵活性（比如支持多种预分词器、后处理规则等）。\n",
    "- 生态支持：tokenizers 是 transformers 生态的一部分，训练好的分词器可以直接与 transformers 的模型（如 BERT、T5）结合使用。\n",
    "\n",
    "---\n",
    "\n",
    "## 加载和使用训练好的分词器\n",
    "如果你想在后续任务中加载这个分词器，可以用以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', '你好', '，', '世界', '！']\n",
      "tensor([[14914,    16, 10314,     5, 13842,  9224,  9878,  9213]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 加载训练好的分词器\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"my_tokenizer.json\")\n",
    "\n",
    "# 分词\n",
    "text = \"Hello, world! 你好，世界！\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# 编码为 ID（适合模型输入）\n",
    "encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "print(encoded[\"input_ids\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
