{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1000000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\", \"en-zh\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Sixty-first session', 'zh': '第六十一届会议'}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "class T5Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "class PositionalEncoding(nn.Module): \n",
    "    def __init__(self, d_model, max_len=5000): \n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len].to(x.device)\n",
    "\n",
    "\n",
    "pe = PositionalEncoding(512)\n",
    "x = torch.randn(size=(2,5,512))\n",
    "# print(x[1,0,:])\n",
    "\n",
    "[para.numel() for para in pe.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旋转位置编码\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module): \n",
    "    # 旋转位置编码实现\n",
    "    def __init__(self, dim: int, max_position: int = 10000): \n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0, \"dim必须是偶数\"\n",
    "        self.dim = dim\n",
    "        self.max_position = max_position\n",
    "        \n",
    "        # 预计算频率\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        # 预计算所有位置的cos和sin\n",
    "        self._precompute_embeddings()\n",
    "    \n",
    "    def _precompute_embeddings(self):\n",
    "        \"\"\"预计算最大长度的cos和sin\"\"\"\n",
    "        positions = torch.arange(self.max_position, dtype=torch.float)\n",
    "        freqs = positions[:, None] * self.inv_freq[None, :]  # (max_position, dim/2)\n",
    "        cos = torch.cos(freqs)\n",
    "        sin = torch.sin(freqs)\n",
    "        self.register_buffer(\"cos_cached\", cos, persistent=False)  # (max_position, dim/2)\n",
    "        self.register_buffer(\"sin_cached\", sin, persistent=False)\n",
    "    \n",
    "    def forward(self, seq_len: int, device: torch.device) -> tuple:\n",
    "        \"\"\"\n",
    "        在训练端，我们固定最大尺寸输入是ok的。因为我们为了训练，对传进来的序列长度对齐的。设定了最大长度规则，截断规则。\n",
    "        但是在预测段，最大固定尺寸max_position需要设置的大一些。大部分任务是单样本推理。此时，如果max_position设置过小，可能对生成结果有影响\n",
    "        我们，一般可以和对话最大字符度相同。或者略低。\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"根据序列长度返回对应的cos和sin\"\"\"\n",
    "        assert seq_len <= self.max_position, f\"seq_len ({seq_len}) 超过 max_position ({self.max_position})\"\n",
    "        \n",
    "        # 从缓存中截取需要的部分\n",
    "        cos = self.cos_cached[:seq_len].to(device)\n",
    "        sin = self.sin_cached[:seq_len].to(device)\n",
    "        return cos, sin\n",
    "    \n",
    "        \n",
    "def apply_rotary_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> tuple:\n",
    "    \"\"\"应用旋转位置编码\"\"\"\n",
    "    q_ = q.float()\n",
    "    k_ = k.float()\n",
    "    q_rot = torch.cat([q_[..., :q_.shape[-1]//2] * cos - q_[..., q_.shape[-1]//2:] * sin,\n",
    "                      q_[..., :q_.shape[-1]//2] * sin + q_[..., q_.shape[-1]//2:] * cos], dim=-1)\n",
    "    k_rot = torch.cat([k_[..., :k_.shape[-1]//2] * cos - k_[..., k_.shape[-1]//2:] * sin,\n",
    "                      k_[..., :k_.shape[-1]//2] * sin + k_[..., k_.shape[-1]//2:] * cos], dim=-1)\n",
    "    return q_rot.type_as(q), k_rot.type_as(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, max_position=10000):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model必须被num_heads整除\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)    \n",
    "        self.k_linear = nn.Linear(d_model, d_model)    \n",
    "        self.v_linear = nn.Linear(d_model, d_model)    \n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 初始化旋转位置编码\n",
    "        self.rotary_emb = RotaryEmbedding(self.d_k, max_position) \n",
    "        \n",
    "    # 始终应用掩码（如果 mask 为 None，则传入全 1 掩码） 这样设计的目的是，方便模型后续输出为onnx格式，该格式不建议if for等语句\n",
    "    def forward(self, q, k, v, mask): \n",
    "        batch_size = q.size(0)\n",
    "        seq_len = q.size(1)\n",
    "        \n",
    "        # 线性变换并分割为多头\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 生成旋转位置编码\n",
    "        cos, sin = self.rotary_emb(seq_len, q.device)\n",
    "\n",
    "        # 应用旋转位置编码到q和k\n",
    "        q, k = apply_rotary_emb(q, k, cos, sin)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, v)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.out_linear(output)\n",
    "    \n",
    "\n",
    "attn = MultiHeadAttention(512, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512]\n",
      "1050624\n"
     ]
    }
   ],
   "source": [
    "params = [param.numel() for param in attn.parameters()]\n",
    "print(params)\n",
    "sum = 0\n",
    "for i in params:\n",
    "    sum += i \n",
    "print(sum)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1048576, 2048, 1048576, 512]\n",
      "2099712\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.linear2(torch.relu(self.linear1(x)))\n",
    "    \n",
    "ffn = FeedForward(512, 2048)\n",
    "param = [para.numel() for para in ffn.parameters()]\n",
    "print(param)\n",
    "sum = 0 \n",
    "for i in param:\n",
    "    sum += i\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512, 1048576, 2048, 1048576, 512, 512, 512, 512, 512]\n",
      "3152384\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x \n",
    "\n",
    "encoder_block = EncoderLayer(512, 8, 2048, 0.1)\n",
    "params = [param.numel() for param in encoder_block.parameters()]\n",
    "print(params)\n",
    "sum = 0 \n",
    "for i in params:\n",
    "    sum += i\n",
    "    \n",
    "# 参数量统计\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 1048576, 2048, 1048576, 512, 512, 512, 512, 512, 512, 512]\n",
      "4204032\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model) \n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None): \n",
    "         x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), tgt_mask))\n",
    "         x = x + self.dropout(self.cross_attn(self.norm2(x), enc_output, enc_output, src_mask))\n",
    "         x = x + self.dropout(self.ff(self.norm3(x)))\n",
    "         return x \n",
    "     \n",
    "decoder_block = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "params = [para.numel() for para in decoder_block.parameters()]\n",
    "print(params)\n",
    "sum = 0\n",
    "for i in params:\n",
    "    sum += i\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.embedding.weight ==> 参数量：15360000\n",
      "encoder_layers.0.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.0.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.0.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.0.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.0.norm1.weight ==> 参数量：768\n",
      "encoder_layers.0.norm1.bias ==> 参数量：768\n",
      "encoder_layers.0.norm2.weight ==> 参数量：768\n",
      "encoder_layers.0.norm2.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.1.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.1.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.1.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.1.norm1.weight ==> 参数量：768\n",
      "encoder_layers.1.norm1.bias ==> 参数量：768\n",
      "encoder_layers.1.norm2.weight ==> 参数量：768\n",
      "encoder_layers.1.norm2.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.2.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.2.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.2.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.2.norm1.weight ==> 参数量：768\n",
      "encoder_layers.2.norm1.bias ==> 参数量：768\n",
      "encoder_layers.2.norm2.weight ==> 参数量：768\n",
      "encoder_layers.2.norm2.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.3.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.3.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.3.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.3.norm1.weight ==> 参数量：768\n",
      "encoder_layers.3.norm1.bias ==> 参数量：768\n",
      "encoder_layers.3.norm2.weight ==> 参数量：768\n",
      "encoder_layers.3.norm2.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.4.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.4.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.4.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.4.norm1.weight ==> 参数量：768\n",
      "encoder_layers.4.norm1.bias ==> 参数量：768\n",
      "encoder_layers.4.norm2.weight ==> 参数量：768\n",
      "encoder_layers.4.norm2.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.5.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.5.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.5.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.5.norm1.weight ==> 参数量：768\n",
      "encoder_layers.5.norm1.bias ==> 参数量：768\n",
      "encoder_layers.5.norm2.weight ==> 参数量：768\n",
      "encoder_layers.5.norm2.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.0.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.0.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.0.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.0.norm1.weight ==> 参数量：768\n",
      "decoder_layers.0.norm1.bias ==> 参数量：768\n",
      "decoder_layers.0.norm2.weight ==> 参数量：768\n",
      "decoder_layers.0.norm2.bias ==> 参数量：768\n",
      "decoder_layers.0.norm3.weight ==> 参数量：768\n",
      "decoder_layers.0.norm3.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.1.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.1.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.1.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.1.norm1.weight ==> 参数量：768\n",
      "decoder_layers.1.norm1.bias ==> 参数量：768\n",
      "decoder_layers.1.norm2.weight ==> 参数量：768\n",
      "decoder_layers.1.norm2.bias ==> 参数量：768\n",
      "decoder_layers.1.norm3.weight ==> 参数量：768\n",
      "decoder_layers.1.norm3.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.2.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.2.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.2.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.2.norm1.weight ==> 参数量：768\n",
      "decoder_layers.2.norm1.bias ==> 参数量：768\n",
      "decoder_layers.2.norm2.weight ==> 参数量：768\n",
      "decoder_layers.2.norm2.bias ==> 参数量：768\n",
      "decoder_layers.2.norm3.weight ==> 参数量：768\n",
      "decoder_layers.2.norm3.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.3.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.3.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.3.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.3.norm1.weight ==> 参数量：768\n",
      "decoder_layers.3.norm1.bias ==> 参数量：768\n",
      "decoder_layers.3.norm2.weight ==> 参数量：768\n",
      "decoder_layers.3.norm2.bias ==> 参数量：768\n",
      "decoder_layers.3.norm3.weight ==> 参数量：768\n",
      "decoder_layers.3.norm3.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.4.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.4.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.4.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.4.norm1.weight ==> 参数量：768\n",
      "decoder_layers.4.norm1.bias ==> 参数量：768\n",
      "decoder_layers.4.norm2.weight ==> 参数量：768\n",
      "decoder_layers.4.norm2.bias ==> 参数量：768\n",
      "decoder_layers.4.norm3.weight ==> 参数量：768\n",
      "decoder_layers.4.norm3.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.5.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.5.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.5.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.5.norm1.weight ==> 参数量：768\n",
      "decoder_layers.5.norm1.bias ==> 参数量：768\n",
      "decoder_layers.5.norm2.weight ==> 参数量：768\n",
      "decoder_layers.5.norm2.bias ==> 参数量：768\n",
      "decoder_layers.5.norm3.weight ==> 参数量：768\n",
      "decoder_layers.5.norm3.bias ==> 参数量：768\n",
      "output_layer.weight ==> 参数量：15360000\n",
      "output_layer.bias ==> 参数量：20000\n",
      "111091232\n"
     ]
    }
   ],
   "source": [
    "# 组合T5模型\n",
    "class T5Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = T5Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 使用了相对位置编码ROPE，这里注释掉去\n",
    "        # self.pos_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src_input, tgt_input, src_mask=None, tgt_mask=None): \n",
    "        # src_emb = self.pos_encoding(self.embedding(src_input)) # 不需要经过绝对位置编码\n",
    "        # tgt_emb = self.pos_encoding(self.embedding(tgt_input)) # 不需要经过绝对位置编码\n",
    "        src_emb = self.embedding(src_input)\n",
    "        tgt_emb = self.embedding(tgt_input)\n",
    "        \n",
    "        enc_output = src_emb \n",
    "        for layer in self.encoder_layers:\n",
    "            enc_output = layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return self.output_layer(dec_output)\n",
    "    \n",
    "t5 = T5Model(20000, 768, 8, 2048, 6, 0.1)\n",
    "\n",
    "sum = 0 \n",
    "\n",
    "for name,param in t5.named_parameters(): \n",
    "    print(f\"{name} ==> 参数量：{param.numel()}\")\n",
    "    sum += param.numel()\n",
    "    \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成词汇表\n",
    "# from generate_tokenizer import gen_tokenizer\n",
    "\n",
    "# # 传入语料库文件， 输出tokenizer的json文件\n",
    "# src_path = \"corpus.txt\"\n",
    "# tokenizer_path = \"translation.json\"\n",
    "# gen_tokenizer(src_path, tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD]\n",
      "特殊 token 映射： {'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}\n",
      "Pad token ID: 0\n",
      "BOS token ID: 2\n",
      "BOS token ID: 2\n",
      "EOS token ID: 3\n",
      "PAD token ID: 0\n",
      "EOS token ID: 3\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 加载 tokenizer，显式设置特殊 token\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"translation.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\"\n",
    ")\n",
    "\n",
    "# 打印特殊 token 和映射\n",
    "print(\"Pad token:\", tokenizer.pad_token)\n",
    "print(\"特殊 token 映射：\", tokenizer.special_tokens_map)\n",
    "print(\"Pad token ID:\", tokenizer.pad_token_id)\n",
    "print(\"BOS token ID:\", tokenizer.convert_tokens_to_ids(\"[BOS]\"))\n",
    "print(\"BOS token ID:\", tokenizer.convert_tokens_to_ids(\"[BOS]\"))\n",
    "print(\"EOS token ID:\", tokenizer.convert_tokens_to_ids(\"[EOS]\"))\n",
    "print(\"PAD token ID:\", tokenizer.convert_tokens_to_ids(\"[PAD]\"))\n",
    "print(\"EOS token ID:\", tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: [2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3]\n",
      "分词结果： ['Hello', ',', 'world', '!', '你好', '，', '世界', '！']\n",
      "Encoded 输出: {'input_ids': tensor([[    2, 14860,    18, 10298,     7, 13791,  9209,  9863,  9198,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Input IDs: tensor([[    2, 14860,    18, 10298,     7, 13791,  9209,  9863,  9198,     3]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "带特殊 token 的分词结果： ['[BOS]', 'Hello', ',', 'world', '!', '你好', '，', '世界', '！', '[EOS]']\n"
     ]
    }
   ],
   "source": [
    "# 输入文本\n",
    "text = \"Hello, world! 你好，世界！\"\n",
    "\n",
    "# 转换为 token ID\n",
    "input_ids = tokenizer.encode(text)\n",
    "print(f\"Token ID: {input_ids}\")\n",
    "\n",
    "# 查看分词后的 token\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"分词结果：\", tokens)\n",
    "\n",
    "encoded = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "print(\"Encoded 输出:\", encoded)\n",
    "\n",
    "# 提取 input_ids 和 attention_mask\n",
    "input_ids = encoded[\"input_ids\"]\n",
    "attention_mask = encoded[\"attention_mask\"]\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "\n",
    "# 转换为 token 查看\n",
    "tokens_with_special = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(\"带特殊 token 的分词结果：\", tokens_with_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID:[2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3]\n",
      "分词结果： ['Hello', ',', 'world', '!', '你好', '，', '世界', '！']\n",
      "Encoded 输出: {'input_ids': [2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Input IDs: [2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 输入文本\n",
    "text = \"Hello, world! 你好，世界！\"\n",
    "\n",
    "# 转换为token ID \n",
    "input_ids = tokenizer.encode(text)\n",
    "print(f\"Token ID:{input_ids}\")\n",
    "\n",
    "# 查看分词后的token\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"分词结果：\", tokens)\n",
    "\n",
    "encoded = tokenizer(\n",
    "    text,                   \n",
    "    # return_tensors=\"pt\",        # 返回 PyTorch 张量（\"tf\" 表示 TensorFlow，None 表示普通列表）\n",
    "    padding=\"max_length\",               # 自动填充（如果处理批量文本）\n",
    "    truncation=True,            # 自动截断（如果超过最大长度）\n",
    "    max_length=512              # 最大序列长度\n",
    ")\n",
    "\n",
    "print(\"Encoded 输出:\", encoded)\n",
    "\n",
    "# 提取 input_ids 和 attention_mask\n",
    "input_ids = encoded[\"input_ids\"]\n",
    "attention_mask = encoded[\"attention_mask\"]\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset): \n",
    "    def __init__(self, dataset, tokenizer, max_length = 10):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = [self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token)]\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        translation = self.dataset[index]['translation']\n",
    "        input = self.tokenizer(\n",
    "            translation[\"en\"],                   \n",
    "            # return_tensors=\"pt\",        # 返回 PyTorch 张量（\"tf\" 表示 TensorFlow，None 表示普通列表）\n",
    "            padding=\"max_length\",               # 自动填充（如果处理批量文本）\n",
    "            truncation=True,            # 自动截断（如果超过最大长度）\n",
    "            max_length=self.max_length              # 最大序列长度\n",
    "        )\n",
    "        output = self.tokenizer(\n",
    "            translation[\"zh\"],                   \n",
    "            # return_tensors=\"pt\",        # 返回 PyTorch 张量（\"tf\" 表示 TensorFlow，None 表示普通列表）\n",
    "            padding=\"max_length\",               # 自动填充（如果处理批量文本）\n",
    "            truncation=True,            # 自动截断（如果超过最大长度）\n",
    "            max_length=self.max_length              # 最大序列长度\n",
    "        )\n",
    "\n",
    "        # 提取 input_ids（去掉 batch 维度）\n",
    "        src_input = input[\"input_ids\"] # [seq_len]\n",
    "        src_attention_mask = input[\"attention_mask\"]\n",
    "        tgt_input = output[\"input_ids\"] # [seq_len] \n",
    "        tgt_output = tgt_input[1:] + self.pad_token_id # 去掉第一个 token（通常是 <BOS>）\n",
    "        \n",
    "        return [src_input, src_attention_mask, tgt_input, tgt_output]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_input, src_attention_mask, tgt_input, tgt_output = zip(*batch)\n",
    "    return (torch.tensor(src_input, dtype=torch.long)\n",
    "            , torch.tensor(src_attention_mask, dtype=torch.long).unsqueeze(1).unsqueeze(2)\n",
    "            , torch.tensor(tgt_input, dtype=torch.long)\n",
    "            , torch.tensor(tgt_output, dtype=torch.long))\n",
    "        \n",
    "train_dataset = TranslationDataset(dataset=dataset[\"train\"], tokenizer=tokenizer, max_length=10)\n",
    "train_loader = DataLoader(train_dataset, batch_size=96, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "\n",
    "model = T5Model(vocab_size, d_model, num_heads, d_ff, num_layers, dropout)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76938496\n"
     ]
    }
   ],
   "source": [
    "param_num = [para.numel() for para in model.parameters()]\n",
    "sum = 0 \n",
    "for n in param_num:\n",
    "    sum += n\n",
    "    \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 1, 1, 10])\n",
      "torch.Size([96, 1, 1, 1, 1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 16035,  1234,  4521,     3,     0,     0,     0,     0,     0],\n",
       "        [    2, 14612,  9848,  2670,  9528,  3559,  9209, 18367,  7558,     3],\n",
       "        [    2,    19,  7538,  7747,  1761,     3,     0,     0,     0,     0],\n",
       "        [    2,    39, 11123, 16281, 10117, 12352, 10691,  9209, 20409,     3],\n",
       "        [    2,  9609, 18931,  9843,  1783,  3350,  5845, 11663, 19702,     3],\n",
       "        [    2, 10105,    20, 14163, 15775, 10316,  1064, 11811,  1381,     3],\n",
       "        [    2,  9553,    13,    89, 13381,    20,     3,     0,     0,     0],\n",
       "        [    2, 11838,  9209,  9940,  1004, 13729, 10024, 14439, 24970,     3],\n",
       "        [    2,  5013,  7978,  1204,  1275,     3,     0,     0,     0,     0],\n",
       "        [    2, 10184, 16468, 26006,  1783, 21213,  6018, 11944, 22163,     3],\n",
       "        [    2,  9922, 10372,  3083, 11898,  9492,  9209,  5472, 22647,     3],\n",
       "        [    2, 18866, 11077,  9209,  3034, 24153, 14747,  1064, 25753,     3],\n",
       "        [    2, 25773, 10159, 22664,  3586,  2218,  9209,  9449, 13104,     3],\n",
       "        [    2,    30,    20, 26255,  9684, 11775,  8175,  8143, 15226,     3],\n",
       "        [    2, 16035, 13710, 19785,  9209,  9609, 14363, 11029, 13572,     3],\n",
       "        [    2,    19, 11580,  9209,  9449, 13799,  3603, 23197,  1064,     3],\n",
       "        [    2,  9439, 15433, 18430,   798, 13077, 13578,  1031, 17515,     3],\n",
       "        [    2,  1180, 28339,  3408,  1741,  3034,     3,     0,     0,     0],\n",
       "        [    2, 10154,    20, 11171,   805,  9686,   806, 17932,  9945,     3],\n",
       "        [    2,    14,    72,    15, 26720, 21751,  9802,  1003, 15913,     3],\n",
       "        [    2, 11083, 11677, 14340,  6149,   799,     3,     0,     0,     0],\n",
       "        [    2,  9439,  9796, 10566,  9733, 11646,  1068,  7054, 10096,     3],\n",
       "        [    2,  9690, 14774,  2402, 11297,  2645, 12284,  2254,  7427,     3],\n",
       "        [    2,  1332,  3475,  2509,  6336,  7419, 11535,     3,     0,     0],\n",
       "        [    2,  7432, 29260,  7431,  5013,  7486,  5013,     3,     0,     0],\n",
       "        [    2, 11459,  9228,  1180,  2408,  6617,  1064,  9209,  4646,     3],\n",
       "        [    2, 14558, 10095, 21343,  9915, 25824, 18349,  1019,  2477,     3],\n",
       "        [    2, 27518,  1106,  4009, 13410,  7424,  2252,     3,     0,     0],\n",
       "        [    2,    19, 15216,   799,     3,     0,     0,     0,     0,     0],\n",
       "        [    2,  2060, 13554, 10250,  9209, 17512, 20826, 10871, 13095,     3],\n",
       "        [    2, 12472,  7551,  1593,  3085, 12904,  7887,  2488,  6021,     3],\n",
       "        [    2,    19,  3138,  3138,    19, 15570,     3,     0,     0,     0],\n",
       "        [    2,  1404,  6832,     3,     0,     0,     0,     0,     0,     0],\n",
       "        [    2, 15484,    20, 14578, 10302, 19680,    14,  9607, 12615,     3],\n",
       "        [    2,    40,    20,  3946,  1007,  2290, 10830, 10541, 24607,     3],\n",
       "        [    2, 18740,  7036, 11093, 14818, 10134, 12255, 23035,  9209,     3],\n",
       "        [    2, 10544, 11851,  1180,     3,     0,     0,     0,     0,     0],\n",
       "        [    2,  1180, 16562,  9466,  9449, 20309, 10021,  9863, 11077,     3],\n",
       "        [    2, 19405,  2252,     7,     3,     0,     0,     0,     0,     0],\n",
       "        [    2, 28424, 16423, 14439,  6451, 11932,  5956, 25678, 20886,     3],\n",
       "        [    2, 19483,  9209,    54, 10111,  9296,  7605,  3604, 13890,     3],\n",
       "        [    2,  3034, 10487, 11988,  1180, 16562, 11677, 29438,  4935,     3],\n",
       "        [    2,  9668,  4842,  2924,    37,    19, 17177, 14805,  7960,     3],\n",
       "        [    2, 26947,  9205, 10672,  5277, 30295,  9206,  1105,  7434,     3],\n",
       "        [    2, 11838,  9209, 24600,  1196, 18716, 13170, 16001, 12375,     3],\n",
       "        [    2,  9796, 13403, 11934, 18816, 11013,  9209, 14131, 18508,     3],\n",
       "        [    2,  1872,  2691,  2060, 22076,  3466,  1477,  5872,  6418,     3],\n",
       "        [    2, 19299,  2472, 14044, 10056, 12665, 10729, 10384,     3,     0],\n",
       "        [    2,  9627,  9762, 10370,  2860,  9599,  9209,  9716,  2480,     3],\n",
       "        [    2,  2248,  1032,  6854, 27064,  2703,  2475,  4935,  7091,     3],\n",
       "        [    2,    24,    20,  9473, 28744, 13648, 16948,  9205, 28744,     3],\n",
       "        [    2,  7432, 16651, 19225,  4935,   805,  1677,  1384,  5865,     3],\n",
       "        [    2, 31393,  2406,  1783, 26955, 15516,  9999, 10604, 11052,     3],\n",
       "        [    2, 12319,  9460,  1629, 15370,  9821, 31533, 17705,  9843,     3],\n",
       "        [    2,    20,  9156,  9127,  9152,  9088,  9196,  9098,  9178,     3],\n",
       "        [    2,  7050, 10479, 12226,  2060, 11056, 21390, 11371, 10169,     3],\n",
       "        [    2, 13594, 25825, 11856,  9209, 11346,  9645, 24320,  2653,     3],\n",
       "        [    2,  4038,  6015,  1783,  7222,  6416,  2813, 10295, 16622,     3],\n",
       "        [    2,  2480,  5472, 11373,  3913,  5336,  6241, 10658,  6302,     3],\n",
       "        [    2,  2764, 12331,  2060, 23069,     3,     0,     0,     0,     0],\n",
       "        [    2, 15359,    20, 11670, 10428, 10083, 28142, 10883, 13242,     3],\n",
       "        [    2, 20177, 11001,  1783, 25368, 10024,  9209,  9640,  9622,     3],\n",
       "        [    2,  1132, 31331,  4930,   131,  2637,  2491,  8229, 11473,     3],\n",
       "        [    2,    14,    72,    15, 22258,  9955, 13802, 10086, 17392,     3],\n",
       "        [    2, 10021, 11575, 10420,  9640, 13219, 10572, 10522, 25524,     3],\n",
       "        [    2,  7036, 18503, 10904, 21889, 29588, 12916, 12436, 18741,     3],\n",
       "        [    2,  9205,    74,  9206,  2060,  9887,  2860, 10367, 12919,     3],\n",
       "        [    2,    14, 15665,    24,    14,    71, 15956,     3,     0,     0],\n",
       "        [    2,  1687,  2917, 10222,  9209,  1669,  4329, 10224, 15377,     3],\n",
       "        [    2,  2254,  3440,  8309,  1873,     3,     0,     0,     0,     0],\n",
       "        [    2, 10749, 31430, 12112, 11723,    27,  3875,  9209, 21608,     3],\n",
       "        [    2, 10159,  9209,  9827, 12227,  9556,  9802,  1089, 14418,     3],\n",
       "        [    2, 11128,    20,  9439, 20826, 11379, 13075, 11503,  9223,     3],\n",
       "        [    2,    14,  4752,  1244, 10525,    15,  1111,  9759,  1515,     3],\n",
       "        [    2, 24489,  3285, 13395, 16928,  5108,  9478,  1646,  4646,     3],\n",
       "        [    2, 11864, 17388, 10479, 22839, 10940,  3060,  1484,     3,     0],\n",
       "        [    2, 24650,  1912,  1676,  3946,  9209, 17140,     3,     0,     0],\n",
       "        [    2, 12108,  1064,  8274, 12777,  4935, 23345, 29212, 18598,     3],\n",
       "        [    2,  1180,  2866,  6134, 11673,  3557,  2073,  9973,   799,     3],\n",
       "        [    2,  9436, 24352,  1064,  3034,  3575,  2949,  5872,  2914,     3],\n",
       "        [    2, 19361,  9466,     3,     0,     0,     0,     0,     0,     0],\n",
       "        [    2,  4105,  2406,  1142,    51, 16994,    20, 11253,    14,     3],\n",
       "        [    2, 12063,    20, 14225,  9627, 10428,  2260,  2676, 11934,     3],\n",
       "        [    2, 30532,  9209,  7942,  6241,  1098,  1032,  3053,  4471,     3],\n",
       "        [    2,  9449, 14032, 20003,  5303, 12904,  6353,  1836,  1040,     3],\n",
       "        [    2, 13005,  7432, 17674, 11029,     3,     0,     0,     0,     0],\n",
       "        [    2, 13651, 11580,  3458,  3064, 28056, 28393,  3034, 12871,     3],\n",
       "        [    2, 11340,  9209, 24205, 31259, 24233,  1412,  3944,  9725,     3],\n",
       "        [    2, 13886,   799,     3,     0,     0,     0,     0,     0,     0],\n",
       "        [    2, 10159,  9209, 10242, 10346,  9733, 12328,   799,     3,     0],\n",
       "        [    2,  6919, 13217,  9228,  6919,  1064, 23794,  1037,  9209,     3],\n",
       "        [    2,  1003, 14511,     3,     0,     0,     0,     0,     0,     0],\n",
       "        [    2,    19,  5872,  9685, 20775,  7564,  6786,  1001,  6756,     3],\n",
       "        [    2,  2102,  8757,  8894,  8530,  8692,  8815,  5429,  8835,     3],\n",
       "        [    2, 20757, 10194, 11321,  1003, 13216, 10222,  9209, 20757,     3],\n",
       "        [    2,    14,    75,    15,  1660,  9783, 10566, 23321,  9478,     3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(next(iter(train_loader))[1].shape)\n",
    "aa = next(iter(train_loader))[1].unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "print(aa.shape)\n",
    "\n",
    "\n",
    "tgt_test = next(iter(train_loader))[2]\n",
    "tgt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_mask(tgt, pad_idx): \n",
    "    tgt_seq_len = tgt.size(1)\n",
    "    tgt_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len))).bool().to(tgt.device)\n",
    "    tgt_mask = tgt_mask & (tgt != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return tgt_mask \n",
    "\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "print(pad_token_id)\n",
    "\n",
    "test_size = tgt_test.shape[1]\n",
    "\n",
    "tgt_mask = torch.tril(torch.ones((test_size, test_size))).bool()\n",
    "tgt_mask = tgt_mask & (tgt_test != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "print(test_size)\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10417/10417 [16:11<00:00, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1, Loss:5.097400858270191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "num_epochs = 1 \n",
    "for epoch in range(num_epochs): \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (src_input, src_attention_mask, tgt_input, tgt_output) in tqdm(train_loader):\n",
    "        src_input = src_input.to(device)\n",
    "        tgt_input = tgt_input.to(device)\n",
    "        tgt_output = tgt_output.to(device)\n",
    "        src_attention_mask = src_attention_mask.to(device)\n",
    "        \n",
    "        tgt_mask = create_mask(tgt_input, pad_token_id)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src_input, tgt_input, src_attention_mask, tgt_mask)\n",
    "        \n",
    "        # test测试是否正确配置项目\n",
    "        # print(f\"tgt_output.shape:{tgt_output.shape}\")      # tgt_output.shape:torch.Size([2, 10])\n",
    "        # print(f\"logits.shape:{logits.shape}\")              # logits.shape:torch.Size([2, 10, 32000])\n",
    "        # break \n",
    "    \n",
    "        loss = loss_fn(logits.view(-1, vocab_size), tgt_output.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs}, Loss:{total_loss / len(train_loader)}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_text, max_length=10000): \n",
    "    model.eval()\n",
    "    input = tokenizer(text=src_text, return_tensors=\"pt\")\n",
    "    src_input = input['input_ids'].to(device)\n",
    "    src_mask_attention = input['attention_mask'].to(device)\n",
    "    print(src_input)\n",
    "    print(src_mask_attention)\n",
    "\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    tgt_ids = [bos_id]\n",
    "    for _ in range(max_length):\n",
    "        tgt_input = torch.tensor([tgt_ids], dtype=torch.long).to(device)\n",
    "        tgt_mask = torch.tril(torch.ones((len(tgt_ids), len(tgt_ids)))).bool().to(device)\n",
    "        tgt_mask = tgt_mask & (tgt_input != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(src_input, tgt_input, src_mask_attention, tgt_mask)\n",
    "            next_token = logits[0, -1].argmax().item()\n",
    "            \n",
    "        if next_token == eos_id:\n",
    "            break \n",
    "        \n",
    "        tgt_ids.append(next_token)\n",
    "        \n",
    "\n",
    "    return ''.join(tokenizer.convert_ids_to_tokens(tgt_ids)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,  9941,  9313,    71, 13826,     7,     3]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m src_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a test!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m翻译结果：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslated\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 19\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(model, src_text, max_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m tgt_mask \u001b[38;5;241m&\u001b[39m (tgt_input \u001b[38;5;241m!=\u001b[39m pad_token_id)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_token \u001b[38;5;241m==\u001b[39m eos_id:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m, in \u001b[0;36mT5Model.forward\u001b[0;34m(self, src_input, tgt_input, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m tgt_emb\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[0;32m---> 26\u001b[0m     dec_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(dec_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, enc_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, enc_output, src_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tgt_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m): \n\u001b[1;32m     13\u001b[0m      x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), tgt_mask))\n\u001b[0;32m---> 14\u001b[0m      x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m      x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x)))\n\u001b[1;32m     16\u001b[0m      \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(seq_len, q\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 应用旋转位置编码到q和k\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[1;32m     34\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(q, k, cos, sin)\u001b[0m\n\u001b[1;32m     48\u001b[0m k_ \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     49\u001b[0m q_rot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([q_[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :q_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m cos \u001b[38;5;241m-\u001b[39m q_[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, q_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m*\u001b[39m sin,\n\u001b[1;32m     50\u001b[0m                   q_[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :q_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m sin \u001b[38;5;241m+\u001b[39m q_[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, q_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m*\u001b[39m cos], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m k_rot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mk_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mk_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m \u001b[38;5;241m-\u001b[39m k_[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, k_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m*\u001b[39m sin,\n\u001b[1;32m     52\u001b[0m                   k_[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :k_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m sin \u001b[38;5;241m+\u001b[39m k_[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, k_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m*\u001b[39m cos], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_rot\u001b[38;5;241m.\u001b[39mtype_as(q), k_rot\u001b[38;5;241m.\u001b[39mtype_as(k)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "src_text = \"This is a test!\"\n",
    "\n",
    "translated = translate(model, src_text)\n",
    "\n",
    "print(f\"翻译结果：{translated}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
