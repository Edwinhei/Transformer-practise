{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自然语言处理（NLP）中，分词（Tokenization）是文本预处理的核心步骤，它将连续的文本分解为离散的单元（称为“tokens”），如单词、子词或字符，以便计算机能够理解和处理。分词技术的选择直接影响后续任务（如文本分类、机器翻译、文本生成等）的性能。现阶段，随着NLP技术的快速发展，流行的分词技术涵盖了传统方法、子词分词技术、深度学习方法以及预训练语言模型内置的分词技术。以下是对这些技术的详细讲解：\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 传统分词技术\n",
    "\n",
    "传统分词技术主要依赖规则、统计或词典，虽然在现代NLP中逐渐被更先进的方法取代，但在特定场景下仍有应用。\n",
    "\n",
    "### 1.1 基于规则的分词（Rule-based Tokenization）\n",
    "\n",
    "- **原理**：通过预定义的规则识别单词边界。\n",
    "  - 对于英语，通常以空格和标点符号作为分隔符。\n",
    "  - 对于中文等没有自然分隔符的语言，可能依赖语法规则或简单的词典匹配。\n",
    "- **优点**：\n",
    "  - 实现简单，计算成本低。\n",
    "  - 对于规则明确的语言（如英语）效果较好。\n",
    "- **缺点**：\n",
    "  - 难以处理歧义和未登录词（Out-of-Vocabulary, OOV）。\n",
    "  - 对语言依赖性强，适应性差。\n",
    "- **应用场景**：早期NLP系统、规则明确的文本处理任务。\n",
    "\n",
    "### 1.2 统计分词（Statistical Tokenization）\n",
    "- **原理**：利用统计模型计算不同分词序列的概率，选择最优的分词方式。\n",
    "  - 常见模型如N-gram，通过分析词或字符的共现频率来确定分词边界。\n",
    "- **优点**：\n",
    "  - 能够处理一定程度的歧义。\n",
    "  - 比纯规则方法更灵活。\n",
    "- **缺点**：\n",
    "  - 需要大量标注数据训练模型。\n",
    "  - 计算复杂度较高。\n",
    "- **应用场景**：中文分词、语言模型训练等。\n",
    "\n",
    "### 1.3 词典分词（Dictionary-based Tokenization）\n",
    "- **原理**：使用预构建的词典匹配文本中的单词。\n",
    "  - **方法**：如前向最大匹配（FMM，从左到右匹配最长词）、后向最大匹配（BMM，从右到左匹配）。\n",
    "- **优点**：\n",
    "  - 简单高效，适用于词典完备的场景。\n",
    "- **缺点**：\n",
    "  - 无法处理未登录词或新词。\n",
    "  - 词典维护成本高。\n",
    "- **应用场景**：中文分词、命名实体识别等。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 子词分词技术（Subword Tokenization）\n",
    "子词分词是近年来NLP领域的重大突破，尤其在处理未登录词、多语言任务和形态变化丰富的语言时表现出色。它将单词分解为更小的单元（如子词），既保留了一定的语义信息，又减小了词汇表规模。\n",
    "\n",
    "### 2.1 Byte Pair Encoding（BPE）\n",
    "- **原理**：最初用于数据压缩，后被引入NLP。\n",
    "  - 1. 将文本分解为字符序列。\n",
    "  - 2. 统计相邻字符对的出现频率。\n",
    "  - 3. 合并频率最高的字符对，形成新的子词单元。\n",
    "  - 4. 重复此过程，直到达到预设的词汇表大小。\n",
    "- **示例**：单词“unbelievable”可能被分解为“un”、“##believ”、“##able”。\n",
    "- **优点**：\n",
    "  - 有效处理未登录词。\n",
    "  - 减小词汇表规模，提升模型效率。\n",
    "- **应用**：GPT系列模型、RoBERTa等。\n",
    "- **变种**：SentencePiece（支持无监督学习和多语言）。\n",
    "\n",
    "### 2.2 WordPiece\n",
    "- **原理**：与BPE类似，但合并子词时基于语言模型概率。\n",
    "  - 选择能最大化语言模型概率的字符对进行合并。\n",
    "- **优点**：\n",
    "  - 生成的子词更具语义信息。\n",
    "  - 与掩码语言模型（MLM）任务结合紧密。\n",
    "- **应用**：BERT、DistilBERT等模型。\n",
    "\n",
    "### 2.3 Unigram Language Model（Unigram LM）\n",
    "- **原理**：基于Unigram语言模型选择最优的子词集合。\n",
    "  - 从一个较大的子词集合开始，逐步删除对模型影响最小的子词，直到满足词汇表大小要求。\n",
    "- **优点**：\n",
    "  - 更灵活，能捕捉语言的统计特性。\n",
    "  - 适用于多语言场景。\n",
    "- **应用**：ALBERT、T5等模型。\n",
    "\n",
    "### 2.4 字符级分词（Character-level Tokenization）\n",
    "- **原理**：将文本分解为单个字符。\n",
    "- **优点**：\n",
    "  - 完全避免未登录词问题。\n",
    "  - 词汇表极小，适合资源受限场景。\n",
    "- **缺点**：\n",
    "  - 丢失词级语义信息。\n",
    "  - 序列长度增加，计算成本高。\n",
    "- **应用**：拼写纠错、语音识别等特定任务。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 深度学习分词技术\n",
    "\n",
    "深度学习的发展推动了分词技术的智能化，神经网络模型能够自动学习分词模式，减少人工干预。\n",
    "\n",
    "### 3.1 神经网络分词（Neural Tokenization）\n",
    "\n",
    "- **原理**：将分词视为序列标注任务，使用神经网络模型（如RNN、Transformer）预测每个字符的边界。\n",
    "  - 常用标签：BILOU（Begin, Inside, Last, Outside, Unit）。\n",
    "- **优点**：\n",
    "  - 自动学习特征，无需手动设计规则。\n",
    "  - 能处理复杂的语言现象和歧义。\n",
    "- **缺点**：\n",
    "  - 需要大量标注数据。\n",
    "  - 训练和推理成本高。\n",
    "- **应用**：中文分词、命名实体识别等。\n",
    "\n",
    "### 3.2 预训练语言模型的分词\n",
    "\n",
    "- **原理**：现代预训练语言模型内置了特定的分词技术，并通过大规模语料学习语言知识。\n",
    "  - **BERT**：使用WordPiece分词。\n",
    "  - **GPT**：使用BPE分词。\n",
    "- **优点**：\n",
    "  - 分词与模型训练深度结合，性能优异。\n",
    "  - 能捕捉丰富的上下文信息。\n",
    "- **缺点**：\n",
    "  - 模型庞大，计算资源需求高。\n",
    "- **应用**：文本分类、生成、翻译等几乎所有NLP任务。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 多语言和跨语言分词技术\n",
    "\n",
    "随着多语言需求的增加，语言无关的分词技术受到关注。\n",
    "\n",
    "### 4.1 SentencePiece\n",
    "- **原理**：一种无监督、语言无关的分词工具，支持BPE和Unigram LM算法。\n",
    "  - 直接处理原始文本，无需预分词。\n",
    "- **优点**：\n",
    "  - 适用于多语言任务。\n",
    "  - 能处理未登录词。\n",
    "- **应用**：T5、mBART等模型。\n",
    "\n",
    "### 4.2 XLM-Roberta的分词\n",
    "- **原理**：使用SentencePiece的BPE变种，在100多种语言上训练。\n",
    "- **优点**：\n",
    "  - 支持跨语言迁移学习。\n",
    "- **应用**：多语言文本分类、命名实体识别等。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 分词技术的选择标准\n",
    "\n",
    "选择分词技术时需综合考虑以下因素：\n",
    "\n",
    "- **语言特性**：英语（有分隔符）与中文（无分隔符）需求不同。\n",
    "- **任务需求**：翻译需要细粒度分词，分类可能需要词级分词。\n",
    "- **数据量**：大数据支持复杂模型，小数据适合简单方法。\n",
    "- **计算资源**：子词分词能优化资源使用。\n",
    "- **未登录词处理**：子词和字符级分词更具优势。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 总结\n",
    "\n",
    "现阶段流行的分词技术包括：\n",
    "\n",
    "- **传统技术**：基于规则、统计和词典，适合特定场景。\n",
    "- **子词分词**：BPE、WordPiece、Unigram LM，广泛应用于现代模型。\n",
    "- **深度学习分词**：神经网络自动学习，性能优异。\n",
    "- **预训练语言模型分词**：与BERT、GPT等结合，效果显著。\n",
    "- **多语言分词**：如SentencePiece，支持跨语言任务。\n",
    "\n",
    "每种技术各有优劣，选择时需根据具体任务、语言和资源综合权衡。未来，随着NLP技术的发展，分词方法可能会更加智能化和自适应，进一步提升性能和适用性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SentencePiece分词实战\n",
    "\n",
    "以下是从提供数据集到完成数据集分词，并保存分词配置文件和词汇表（vocab）的完整流程。我将以中英文混合数据集为例，使用SentencePiece作为分词工具，逐步说明每个步骤，并提供可运行的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.提供并准备数据集\n",
    "\n",
    "为了完成分词任务，首先需要一个文本数据集。我将使用Hugging Face的`datasets`库加载一个公开的英语-中文翻译数据集`opus100`，并从中提取中英文文本作为训练语料。\n",
    "\n",
    "#### 1.1 安装依赖\n",
    "确保你的环境中已安装必要的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/edwin/miniconda3/envs/pytorch/lib/python3.11/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 加载并保存数据集\n",
    "我们将加载opus100数据集的英语-中文部分，并将文本保存到一个文件中，作为分词模型的训练语料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集已保存到corpus.txt\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载opus100的en-zh部分\n",
    "dataset = load_dataset(\"opus100\", \"en-zh\")\n",
    "\n",
    "# 提取训练集\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# 将英文和中文文本保存到文件\n",
    "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f: \n",
    "    for item in train_data:\n",
    "        f.write(item[\"translation\"][\"en\"] + \"\\n\")\n",
    "        f.write(item[\"translation\"][\"zh\"] + \"\\n\")\n",
    "        \n",
    "print(\"数据集已保存到corpus.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **输出文件**：corpus.txt 是一个纯文本文件，每行是一个英文或中文句子，包含了训练集中所有的中英文文本。\n",
    "- **注意**：确保有足够的磁盘空间，因为opus100数据集较大。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 训练分词模型\n",
    "接下来，使用SentencePiece训练一个BPE（Byte Pair Encoding）分词模型。训练后会生成模型文件和词汇表文件。\n",
    "\n",
    "#### 2.1 训练SentencePiece模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 1. 加载语料到内存\n",
    "def load_corpus_to_memory(file_path):\n",
    "    start_time = time.time()\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"已加载 {len(lines)} 行到内存，耗时 {time.time() - start_time:.2f} 秒\")\n",
    "    return lines\n",
    "\n",
    "# 2. 写入内存文件系统\n",
    "def write_to_memory_filesystem(corpus, filename=\"corpus_mem.txt\"):\n",
    "    # 使用 /dev/shm 作为内存文件系统（Linux 系统）\n",
    "    mem_fs_path = os.path.join(\"/dev/shm\", filename)\n",
    "    start_time = time.time()\n",
    "    with open(mem_fs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(corpus)\n",
    "    print(f\"已将语料写入内存文件系统 {mem_fs_path}，耗时 {time.time() - start_time:.2f} 秒\")\n",
    "    return mem_fs_path\n",
    "\n",
    "# 3. 主函数\n",
    "def main():\n",
    "    # 加载语料到内存\n",
    "    corpus = load_corpus_to_memory(\"corpus.txt\")\n",
    "    \n",
    "    # 为了快速体验，只用前 5 万行\n",
    "    # corpus = corpus[:200000]\n",
    "    print(f\"使用 {len(corpus)} 行进行训练\")\n",
    "    \n",
    "    # 写入内存文件系统\n",
    "    mem_file_path = write_to_memory_filesystem(corpus)\n",
    "    \n",
    "    # 训练模型\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=mem_file_path,\n",
    "            model_prefix=\"mymodel\",\n",
    "            vocab_size=32000,  # 词汇表大小\n",
    "            model_type=\"bpe\",\n",
    "            character_coverage=0.9995,\n",
    "            num_threads=20,  # 充分利用 20 个核心\n",
    "            train_extremely_large_corpus=True\n",
    "        )\n",
    "        print(f\"分词模型训练完成，耗时 {time.time() - start_time:.2f} 秒\")\n",
    "        print(\"分词模型已保存为 mymodel.model 和 mymodel.vocab\")\n",
    "    finally:\n",
    "        # 删除内存文件系统中的文件\n",
    "        os.unlink(mem_file_path)\n",
    "        print(f\"已删除内存文件 {mem_file_path}\")\n",
    "\n",
    "    # 测试分词效果\n",
    "    sp = spm.SentencePieceProcessor(model_file=\"mymodel.model\")\n",
    "    text = \"Hello, world! 你好，世界！\"\n",
    "    tokens = sp.encode(text, out_type=str)\n",
    "    print(\"\\n分词结果：\")\n",
    "    print(f\"混合文本: {tokens}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hellohappy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([\"hello\", \"happy\",])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数说明：\n",
    "  - `input`：训练用的语料文件。\n",
    "  - `model_prefix`：生成的文件前缀，结果为 mymodel.model（模型文件）和 mymodel.vocab（词汇表文件）。\n",
    "  - `vocab_size`：词汇表大小，32000 是一个适用于中英文的常见选择。\n",
    "  - `model_type`：选择 BPE 算法，适合多语言分词。\n",
    "  - `character_coverage`：确保覆盖 99.95% 的字符，适用于中英文混合场景。\n",
    "- 输出文件：\n",
    "  - `mymodel.model`：训练好的分词模型。\n",
    "  - `mymodel.vocab`：词汇表文件，每行是一个 token 及其得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
