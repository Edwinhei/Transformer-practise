{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1000000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\", \"en-zh\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Sixty-first session', 'zh': '第六十一届会议'}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "class T5Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "class PositionalEncoding(nn.Module): \n",
    "    def __init__(self, d_model, max_len=5000): \n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len].to(x.device)\n",
    "\n",
    "\n",
    "pe = PositionalEncoding(512)\n",
    "x = torch.randn(size=(2,5,512))\n",
    "# print(x[1,0,:])\n",
    "\n",
    "[para.numel() for para in pe.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旋转位置编码\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module): \n",
    "    # 旋转位置编码实现\n",
    "    def __init__(self, dim: int, max_position: int = 10000): \n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0, \"dim必须是偶数\"\n",
    "        self.dim = dim\n",
    "        self.max_position = max_position\n",
    "        \n",
    "        # 预计算频率\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        # 预计算所有位置的cos和sin\n",
    "        self._precompute_embeddings()\n",
    "    \n",
    "    def _precompute_embeddings(self):\n",
    "        \"\"\"预计算最大长度的cos和sin\"\"\"\n",
    "        positions = torch.arange(self.max_position, dtype=torch.float)\n",
    "        freqs = positions[:, None] * self.inv_freq[None, :]  # (max_position, dim/2)\n",
    "        cos = torch.cos(freqs)\n",
    "        sin = torch.sin(freqs)\n",
    "        self.register_buffer(\"cos_cached\", cos, persistent=False)  # (max_position, dim/2)\n",
    "        self.register_buffer(\"sin_cached\", sin, persistent=False)\n",
    "    \n",
    "    def forward(self, seq_len: int, device: torch.device) -> tuple:\n",
    "        \"\"\"\n",
    "        在训练端，我们固定最大尺寸输入是ok的。因为我们为了训练，对传进来的序列长度对齐的。设定了最大长度规则，截断规则。\n",
    "        但是在预测段，最大固定尺寸max_position需要设置的大一些。大部分任务是单样本推理。此时，如果max_position设置过小，可能对生成结果有影响\n",
    "        我们，一般可以和对话最大字符度相同。或者略低。\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"根据序列长度返回对应的cos和sin\"\"\"\n",
    "        assert seq_len <= self.max_position, f\"seq_len ({seq_len}) 超过 max_position ({self.max_position})\"\n",
    "        \n",
    "        # 从缓存中截取需要的部分\n",
    "        cos = self.cos_cached[:seq_len].to(device)\n",
    "        sin = self.sin_cached[:seq_len].to(device)\n",
    "        return cos, sin\n",
    "    \n",
    "# 在单向or双向自注意力时可以这样操作。因为q，k同维度，当涉及交叉注意力时，此时就不能这么操作了，在训练时，由于我们设置max_length都一样长，可能并无感知。\n",
    "# 但在训练时，编码端长度和解码端长度大多数情况下是未对齐的，此时就会报错。  \n",
    "def apply_rotary_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> tuple:\n",
    "    \"\"\"应用旋转位置编码\"\"\"\n",
    "    q_ = q.float()\n",
    "    k_ = k.float()\n",
    "    q_rot = torch.cat([q_[..., :q_.shape[-1]//2] * cos - q_[..., q_.shape[-1]//2:] * sin,\n",
    "                      q_[..., :q_.shape[-1]//2] * sin + q_[..., q_.shape[-1]//2:] * cos], dim=-1)\n",
    "    k_rot = torch.cat([k_[..., :k_.shape[-1]//2] * cos - k_[..., k_.shape[-1]//2:] * sin,\n",
    "                      k_[..., :k_.shape[-1]//2] * sin + k_[..., k_.shape[-1]//2:] * cos], dim=-1)\n",
    "    return q_rot.type_as(q), k_rot.type_as(k)\n",
    "\n",
    "# 在交叉注意力时，由于q，k的长度不一致。如果我们用常规的ROPE就不行，维度没有对齐。因此，在交叉注意力计算时，我们需要计算各自的ROPE值\n",
    "def apply_rotary_emb_single(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"应用旋转位置编码到单个张量\"\"\"\n",
    "    x_ = x.float()\n",
    "    d = x_.shape[-1]\n",
    "    x_rot = torch.cat([\n",
    "        x_[..., :d//2] * cos - x_[..., d//2:] * sin,\n",
    "        x_[..., :d//2] * sin + x_[..., d//2:] * cos\n",
    "    ], dim=-1)\n",
    "    return x_rot.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, max_position=10000):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model必须被num_heads整除\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)    \n",
    "        self.k_linear = nn.Linear(d_model, d_model)    \n",
    "        self.v_linear = nn.Linear(d_model, d_model)    \n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 初始化旋转位置编码\n",
    "        self.rotary_emb = RotaryEmbedding(self.d_k, max_position) \n",
    "        \n",
    "    # 这个注意力前向传播需要调整下，由于我们编解码类型，在交叉注意力计算时，由于q，k维度不一致。不能套用常规的ROPE编码\n",
    "    # def forward(self, q, k, v, mask): \n",
    "    #     batch_size = q.size(0)\n",
    "    #     seq_len = q.size(1)\n",
    "        \n",
    "    #     # 线性变换并分割为多头\n",
    "    #     q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    #     k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    #     v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    #     # 生成旋转位置编码\n",
    "    #     cos, sin = self.rotary_emb(seq_len, q.device)\n",
    "\n",
    "\n",
    "    #     # 应用旋转位置编码到q和k\n",
    "    #     q, k = apply_rotary_emb(q, k, cos, sin)\n",
    "\n",
    "    #     scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "    #     # 在float16中-1e9溢出，这里将其改为-1e4\n",
    "    #     scores = scores.masked_fill(mask==0, -1e4)\n",
    "        \n",
    "    #     attention = torch.softmax(scores, dim=-1)\n",
    "    #     output = torch.matmul(attention, v)\n",
    "        \n",
    "    #     output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "    #     return self.out_linear(output)\n",
    "    \n",
    "    \n",
    "    # 始终应用掩码（如果 mask 为 None，则传入全 1 掩码） 这样设计的目的是，方便模型后续输出为onnx格式，该格式不建议if for等语句\n",
    "    def forward(self, q, k, v, mask): \n",
    "        batch_size = q.size(0)\n",
    "        q_seq_len = q.size(1)  # q 的序列长度\n",
    "        k_seq_len = k.size(1)  # k 的序列长度\n",
    "        \n",
    "        # 线性变换并分割为多头\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 为 q 和 k 分别生成旋转位置编码\n",
    "        cos_q, sin_q = self.rotary_emb(q_seq_len, q.device)\n",
    "        cos_k, sin_k = self.rotary_emb(k_seq_len, k.device)\n",
    "        \n",
    "        # 分别应用旋转位置编码\n",
    "        q = apply_rotary_emb_single(q, cos_q, sin_q)\n",
    "        k = apply_rotary_emb_single(k, cos_k, sin_k)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # 在float16中-1e9溢出，这里将其改为-1e4\n",
    "        scores = scores.masked_fill(mask==0, -1e4)\n",
    "        \n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, v)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.out_linear(output)\n",
    "    \n",
    "\n",
    "attn = MultiHeadAttention(512, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512]\n",
      "1050624\n"
     ]
    }
   ],
   "source": [
    "params = [param.numel() for param in attn.parameters()]\n",
    "print(params)\n",
    "sum = 0\n",
    "for i in params:\n",
    "    sum += i \n",
    "print(sum)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1048576, 2048, 1048576, 512]\n",
      "2099712\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.linear2(torch.relu(self.linear1(x)))\n",
    "    \n",
    "ffn = FeedForward(512, 2048)\n",
    "param = [para.numel() for para in ffn.parameters()]\n",
    "print(param)\n",
    "sum = 0 \n",
    "for i in param:\n",
    "    sum += i\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512, 1048576, 2048, 1048576, 512, 512, 512, 512, 512]\n",
      "3152384\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x \n",
    "\n",
    "encoder_block = EncoderLayer(512, 8, 2048, 0.1)\n",
    "params = [param.numel() for param in encoder_block.parameters()]\n",
    "print(params)\n",
    "sum = 0 \n",
    "for i in params:\n",
    "    sum += i\n",
    "    \n",
    "# 参数量统计\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 1048576, 2048, 1048576, 512, 512, 512, 512, 512, 512, 512]\n",
      "4204032\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model) \n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None): \n",
    "         x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), tgt_mask))\n",
    "         x = x + self.dropout(self.cross_attn(self.norm2(x), enc_output, enc_output, src_mask))\n",
    "         x = x + self.dropout(self.ff(self.norm3(x)))\n",
    "         return x \n",
    "     \n",
    "decoder_block = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "params = [para.numel() for para in decoder_block.parameters()]\n",
    "print(params)\n",
    "sum = 0\n",
    "for i in params:\n",
    "    sum += i\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.embedding.weight ==> 参数量：15360000\n",
      "encoder_layers.0.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.0.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.0.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.0.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.0.norm1.weight ==> 参数量：768\n",
      "encoder_layers.0.norm1.bias ==> 参数量：768\n",
      "encoder_layers.0.norm2.weight ==> 参数量：768\n",
      "encoder_layers.0.norm2.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.1.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.1.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.1.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.1.norm1.weight ==> 参数量：768\n",
      "encoder_layers.1.norm1.bias ==> 参数量：768\n",
      "encoder_layers.1.norm2.weight ==> 参数量：768\n",
      "encoder_layers.1.norm2.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.2.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.2.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.2.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.2.norm1.weight ==> 参数量：768\n",
      "encoder_layers.2.norm1.bias ==> 参数量：768\n",
      "encoder_layers.2.norm2.weight ==> 参数量：768\n",
      "encoder_layers.2.norm2.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.3.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.3.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.3.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.3.norm1.weight ==> 参数量：768\n",
      "encoder_layers.3.norm1.bias ==> 参数量：768\n",
      "encoder_layers.3.norm2.weight ==> 参数量：768\n",
      "encoder_layers.3.norm2.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.4.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.4.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.4.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.4.norm1.weight ==> 参数量：768\n",
      "encoder_layers.4.norm1.bias ==> 参数量：768\n",
      "encoder_layers.4.norm2.weight ==> 参数量：768\n",
      "encoder_layers.4.norm2.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.5.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.5.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.5.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.5.norm1.weight ==> 参数量：768\n",
      "encoder_layers.5.norm1.bias ==> 参数量：768\n",
      "encoder_layers.5.norm2.weight ==> 参数量：768\n",
      "encoder_layers.5.norm2.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.0.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.0.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.0.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.0.norm1.weight ==> 参数量：768\n",
      "decoder_layers.0.norm1.bias ==> 参数量：768\n",
      "decoder_layers.0.norm2.weight ==> 参数量：768\n",
      "decoder_layers.0.norm2.bias ==> 参数量：768\n",
      "decoder_layers.0.norm3.weight ==> 参数量：768\n",
      "decoder_layers.0.norm3.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.1.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.1.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.1.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.1.norm1.weight ==> 参数量：768\n",
      "decoder_layers.1.norm1.bias ==> 参数量：768\n",
      "decoder_layers.1.norm2.weight ==> 参数量：768\n",
      "decoder_layers.1.norm2.bias ==> 参数量：768\n",
      "decoder_layers.1.norm3.weight ==> 参数量：768\n",
      "decoder_layers.1.norm3.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.2.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.2.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.2.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.2.norm1.weight ==> 参数量：768\n",
      "decoder_layers.2.norm1.bias ==> 参数量：768\n",
      "decoder_layers.2.norm2.weight ==> 参数量：768\n",
      "decoder_layers.2.norm2.bias ==> 参数量：768\n",
      "decoder_layers.2.norm3.weight ==> 参数量：768\n",
      "decoder_layers.2.norm3.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.3.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.3.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.3.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.3.norm1.weight ==> 参数量：768\n",
      "decoder_layers.3.norm1.bias ==> 参数量：768\n",
      "decoder_layers.3.norm2.weight ==> 参数量：768\n",
      "decoder_layers.3.norm2.bias ==> 参数量：768\n",
      "decoder_layers.3.norm3.weight ==> 参数量：768\n",
      "decoder_layers.3.norm3.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.4.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.4.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.4.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.4.norm1.weight ==> 参数量：768\n",
      "decoder_layers.4.norm1.bias ==> 参数量：768\n",
      "decoder_layers.4.norm2.weight ==> 参数量：768\n",
      "decoder_layers.4.norm2.bias ==> 参数量：768\n",
      "decoder_layers.4.norm3.weight ==> 参数量：768\n",
      "decoder_layers.4.norm3.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.5.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.5.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.5.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.5.norm1.weight ==> 参数量：768\n",
      "decoder_layers.5.norm1.bias ==> 参数量：768\n",
      "decoder_layers.5.norm2.weight ==> 参数量：768\n",
      "decoder_layers.5.norm2.bias ==> 参数量：768\n",
      "decoder_layers.5.norm3.weight ==> 参数量：768\n",
      "decoder_layers.5.norm3.bias ==> 参数量：768\n",
      "output_layer.weight ==> 参数量：15360000\n",
      "output_layer.bias ==> 参数量：20000\n",
      "111091232\n"
     ]
    }
   ],
   "source": [
    "# 组合T5模型\n",
    "class T5Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = T5Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 使用了相对位置编码ROPE，这里注释掉去\n",
    "        # self.pos_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src_input, tgt_input, src_mask=None, tgt_mask=None): \n",
    "        # src_emb = self.pos_encoding(self.embedding(src_input)) # 不需要经过绝对位置编码\n",
    "        # tgt_emb = self.pos_encoding(self.embedding(tgt_input)) # 不需要经过绝对位置编码\n",
    "        src_emb = self.embedding(src_input)\n",
    "        tgt_emb = self.embedding(tgt_input)\n",
    "        \n",
    "        enc_output = src_emb \n",
    "        for layer in self.encoder_layers:\n",
    "            enc_output = layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return self.output_layer(dec_output)\n",
    "    \n",
    "t5 = T5Model(20000, 768, 8, 2048, 6, 0.1)\n",
    "\n",
    "sum = 0 \n",
    "\n",
    "for name,param in t5.named_parameters(): \n",
    "    print(f\"{name} ==> 参数量：{param.numel()}\")\n",
    "    sum += param.numel()\n",
    "    \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成词汇表\n",
    "# from generate_tokenizer import gen_tokenizer\n",
    "\n",
    "# # 传入语料库文件， 输出tokenizer的json文件\n",
    "# src_path = \"corpus.txt\"\n",
    "# tokenizer_path = \"translation.json\"\n",
    "# gen_tokenizer(src_path, tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD]\n",
      "特殊 token 映射： {'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}\n",
      "Pad token ID: 0\n",
      "BOS token ID: 2\n",
      "BOS token ID: 2\n",
      "EOS token ID: 3\n",
      "PAD token ID: 0\n",
      "EOS token ID: 3\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 加载 tokenizer，显式设置特殊 token\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"translation.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\"\n",
    ")\n",
    "\n",
    "# 打印特殊 token 和映射\n",
    "print(\"Pad token:\", tokenizer.pad_token)\n",
    "print(\"特殊 token 映射：\", tokenizer.special_tokens_map)\n",
    "print(\"Pad token ID:\", tokenizer.pad_token_id)\n",
    "print(\"BOS token ID:\", tokenizer.convert_tokens_to_ids(\"[BOS]\"))\n",
    "print(\"BOS token ID:\", tokenizer.convert_tokens_to_ids(\"[BOS]\"))\n",
    "print(\"EOS token ID:\", tokenizer.convert_tokens_to_ids(\"[EOS]\"))\n",
    "print(\"PAD token ID:\", tokenizer.convert_tokens_to_ids(\"[PAD]\"))\n",
    "print(\"EOS token ID:\", tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: [2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3]\n",
      "分词结果： ['Hello', ',', 'world', '!', '你好', '，', '世界', '！']\n",
      "Encoded 输出: {'input_ids': tensor([[    2, 14860,    18, 10298,     7, 13791,  9209,  9863,  9198,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Input IDs: tensor([[    2, 14860,    18, 10298,     7, 13791,  9209,  9863,  9198,     3]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "带特殊 token 的分词结果： ['[BOS]', 'Hello', ',', 'world', '!', '你好', '，', '世界', '！', '[EOS]']\n"
     ]
    }
   ],
   "source": [
    "# 输入文本\n",
    "text = \"Hello, world! 你好，世界！\"\n",
    "\n",
    "# 转换为 token ID\n",
    "input_ids = tokenizer.encode(text)\n",
    "print(f\"Token ID: {input_ids}\")\n",
    "\n",
    "# 查看分词后的 token\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"分词结果：\", tokens)\n",
    "\n",
    "encoded = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "print(\"Encoded 输出:\", encoded)\n",
    "\n",
    "# 提取 input_ids 和 attention_mask\n",
    "input_ids = encoded[\"input_ids\"]\n",
    "attention_mask = encoded[\"attention_mask\"]\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "\n",
    "# 转换为 token 查看\n",
    "tokens_with_special = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(\"带特殊 token 的分词结果：\", tokens_with_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID:[2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3]\n",
      "分词结果： ['Hello', ',', 'world', '!', '你好', '，', '世界', '！']\n",
      "Encoded 输出: {'input_ids': [2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Input IDs: [2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 输入文本\n",
    "text = \"Hello, world! 你好，世界！\"\n",
    "\n",
    "# 转换为token ID \n",
    "input_ids = tokenizer.encode(text)\n",
    "print(f\"Token ID:{input_ids}\")\n",
    "\n",
    "# 查看分词后的token\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"分词结果：\", tokens)\n",
    "\n",
    "encoded = tokenizer(\n",
    "    text,                   \n",
    "    # return_tensors=\"pt\",        # 返回 PyTorch 张量（\"tf\" 表示 TensorFlow，None 表示普通列表）\n",
    "    padding=\"max_length\",               # 自动填充（如果处理批量文本）\n",
    "    truncation=True,            # 自动截断（如果超过最大长度）\n",
    "    max_length=512              # 最大序列长度\n",
    ")\n",
    "\n",
    "print(\"Encoded 输出:\", encoded)\n",
    "\n",
    "# 提取 input_ids 和 attention_mask\n",
    "input_ids = encoded[\"input_ids\"]\n",
    "attention_mask = encoded[\"attention_mask\"]\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset): \n",
    "    def __init__(self, dataset, tokenizer, max_length = 10):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = [self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token)]\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        translation = self.dataset[index]['translation']\n",
    "        input = self.tokenizer(\n",
    "            translation[\"en\"],                   \n",
    "            # return_tensors=\"pt\",        # 返回 PyTorch 张量（\"tf\" 表示 TensorFlow，None 表示普通列表）\n",
    "            padding=\"max_length\",               # 自动填充（如果处理批量文本）\n",
    "            truncation=True,            # 自动截断（如果超过最大长度）\n",
    "            max_length=self.max_length              # 最大序列长度\n",
    "        )\n",
    "        output = self.tokenizer(\n",
    "            translation[\"zh\"],                   \n",
    "            # return_tensors=\"pt\",        # 返回 PyTorch 张量（\"tf\" 表示 TensorFlow，None 表示普通列表）\n",
    "            padding=\"max_length\",               # 自动填充（如果处理批量文本）\n",
    "            truncation=True,            # 自动截断（如果超过最大长度）\n",
    "            max_length=self.max_length              # 最大序列长度\n",
    "        )\n",
    "\n",
    "        # 提取 input_ids（去掉 batch 维度）\n",
    "        src_input = input[\"input_ids\"] # [seq_len]\n",
    "        src_attention_mask = input[\"attention_mask\"]\n",
    "        tgt_input = output[\"input_ids\"] # [seq_len] \n",
    "        tgt_output = tgt_input[1:] + self.pad_token_id # 去掉第一个 token（通常是 <BOS>）\n",
    "        \n",
    "        return [src_input, src_attention_mask, tgt_input, tgt_output]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_input, src_attention_mask, tgt_input, tgt_output = zip(*batch)\n",
    "    return (torch.tensor(src_input, dtype=torch.long)\n",
    "            , torch.tensor(src_attention_mask, dtype=torch.long).unsqueeze(1).unsqueeze(2)\n",
    "            , torch.tensor(tgt_input, dtype=torch.long)\n",
    "            , torch.tensor(tgt_output, dtype=torch.long))\n",
    "        \n",
    "train_dataset = TranslationDataset(dataset=dataset[\"train\"], tokenizer=tokenizer, max_length=10)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "\n",
    "model = T5Model(vocab_size, d_model, num_heads, d_ff, num_layers, dropout)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76938496\n"
     ]
    }
   ],
   "source": [
    "param_num = [para.numel() for para in model.parameters()]\n",
    "sum = 0 \n",
    "for n in param_num:\n",
    "    sum += n\n",
    "    \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 1, 10])\n",
      "torch.Size([512, 1, 1, 1, 1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 24931, 28918,  ...,  1064,     3,     0],\n",
       "        [    2, 13787, 16323,  ..., 15681,  1064,     3],\n",
       "        [    2, 19912,  3305,  ...,  2415,  2694,     3],\n",
       "        ...,\n",
       "        [    2,  1180,  1018,  ...,     0,     0,     0],\n",
       "        [    2, 10589,  9759,  ...,     0,     0,     0],\n",
       "        [    2,  9827, 10316,  ...,  4764, 15473,     3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(next(iter(train_loader))[1].shape)\n",
    "aa = next(iter(train_loader))[1].unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "print(aa.shape)\n",
    "\n",
    "\n",
    "tgt_test = next(iter(train_loader))[2]\n",
    "tgt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[1, 1, 1,  ..., 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1,  ..., 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1,  ..., 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1,  ..., 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1,  ..., 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1,  ..., 1, 1, 0]]]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_mask(tgt, pad_idx): \n",
    "    tgt_seq_len = tgt.size(1)\n",
    "    tgt_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len))).bool().to(tgt.device)\n",
    "    tgt_mask = tgt_mask & (tgt != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return tgt_mask \n",
    "\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "print(pad_token_id)\n",
    "\n",
    "test_size = tgt_test.shape[1]\n",
    "\n",
    "tgt_mask = torch.tril(torch.ones((test_size, test_size))).bool()\n",
    "tgt_mask = tgt_mask & (tgt_test != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "print(test_size)\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1954/1954 [12:23<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10, Loss:5.553257226699938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1954/1954 [11:52<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 10, Loss:4.381652504297215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1954/1954 [11:46<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 10, Loss:3.920534763877931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1954/1954 [11:50<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 10, Loss:3.6301160792123333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1954/1954 [11:47<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 / 10, Loss:3.410952240686212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1954/1954 [12:09<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 / 10, Loss:3.2299106617666924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1954/1954 [12:00<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / 10, Loss:3.0697134004766644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1954/1954 [11:49<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 / 10, Loss:2.926023423122652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1954/1954 [11:49<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 / 10, Loss:2.792363271254478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1954/1954 [11:42<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 10, Loss:2.668927735535613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs): \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (src_input, src_attention_mask, tgt_input, tgt_output) in tqdm(train_loader):\n",
    "        src_input = src_input.to(device)\n",
    "        tgt_input = tgt_input.to(device)\n",
    "        tgt_output = tgt_output.to(device)\n",
    "        src_attention_mask = src_attention_mask.to(device)\n",
    "        \n",
    "        tgt_mask = create_mask(tgt_input, pad_token_id)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            logits = model(src_input, tgt_input, src_attention_mask, tgt_mask)\n",
    "            loss = loss_fn(logits.view(-1, vocab_size), tgt_output.view(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "                \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs}, Loss:{total_loss / len(train_loader)}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_text, max_length=10000): \n",
    "    model.eval()\n",
    "    input = tokenizer(text=src_text, return_tensors=\"pt\")\n",
    "    src_input = input['input_ids'].to(device)\n",
    "    src_mask_attention = input['attention_mask'].to(device)\n",
    "    print(src_input)\n",
    "    print(src_mask_attention)\n",
    "\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    tgt_ids = [bos_id]\n",
    "    for _ in range(max_length):\n",
    "        tgt_input = torch.tensor([tgt_ids], dtype=torch.long).to(device)\n",
    "        tgt_mask = torch.tril(torch.ones((len(tgt_ids), len(tgt_ids)))).bool().to(device)\n",
    "        tgt_mask = tgt_mask & (tgt_input != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16):  # 使用混合精度推理\n",
    "                logits = model(src_input, tgt_input, src_mask_attention, tgt_mask)\n",
    "            next_token = logits[0, -1].argmax().item()\n",
    "            \n",
    "        if next_token == eos_id:\n",
    "            break \n",
    "        \n",
    "        tgt_ids.append(next_token)\n",
    "        \n",
    "\n",
    "    return ''.join(tokenizer.convert_ids_to_tokens(tgt_ids)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 17475,  9313, 10160, 13663,     3]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "翻译结果：好家伙好孩子\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "src_text = \"cat is good boy\"\n",
    "\n",
    "translated = translate(model, src_text)\n",
    "\n",
    "print(f\"翻译结果：{translated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
