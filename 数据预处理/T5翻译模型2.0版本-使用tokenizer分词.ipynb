{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1000000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\", \"en-zh\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Sixty-first session', 'zh': '第六十一届会议'}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "class T5Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "class PositionalEncoding(nn.Module): \n",
    "    def __init__(self, d_model, max_len=5000): \n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len].to(x.device)\n",
    "\n",
    "\n",
    "pe = PositionalEncoding(512)\n",
    "x = torch.randn(size=(2,5,512))\n",
    "# print(x[1,0,:])\n",
    "\n",
    "[para.numel() for para in pe.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model必须被num_heads整除\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)    \n",
    "        self.k_linear = nn.Linear(d_model, d_model)    \n",
    "        self.v_linear = nn.Linear(d_model, d_model)    \n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    # 始终应用掩码（如果 mask 为 None，则传入全 1 掩码）\n",
    "    def forward(self, q, k, v, mask): \n",
    "        batch_size = q.size(0)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, v)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.out_linear(output)\n",
    "    \n",
    "\n",
    "attn = MultiHeadAttention(512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512]\n",
      "1050624\n"
     ]
    }
   ],
   "source": [
    "params = [param.numel() for param in attn.parameters()]\n",
    "print(params)\n",
    "sum = 0\n",
    "for i in params:\n",
    "    sum += i \n",
    "print(sum)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1048576, 2048, 1048576, 512]\n",
      "2099712\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.linear2(torch.relu(self.linear1(x)))\n",
    "    \n",
    "ffn = FeedForward(512, 2048)\n",
    "param = [para.numel() for para in ffn.parameters()]\n",
    "print(param)\n",
    "sum = 0 \n",
    "for i in param:\n",
    "    sum += i\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512, 1048576, 2048, 1048576, 512, 512, 512, 512, 512]\n",
      "3152384\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x \n",
    "\n",
    "encoder_block = EncoderLayer(512, 8, 2048, 0.1)\n",
    "params = [param.numel() for param in encoder_block.parameters()]\n",
    "print(params)\n",
    "sum = 0 \n",
    "for i in params:\n",
    "    sum += i\n",
    "    \n",
    "# 参数量统计\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 262144, 512, 1048576, 2048, 1048576, 512, 512, 512, 512, 512, 512, 512]\n",
      "4204032\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model) \n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None): \n",
    "         x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), tgt_mask))\n",
    "         x = x + self.dropout(self.cross_attn(self.norm2(x), enc_output, enc_output, src_mask))\n",
    "         x = x + self.dropout(self.ff(self.norm3(x)))\n",
    "         return x \n",
    "     \n",
    "decoder_block = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "params = [para.numel() for para in decoder_block.parameters()]\n",
    "print(params)\n",
    "sum = 0\n",
    "for i in params:\n",
    "    sum += i\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.embedding.weight ==> 参数量：15360000\n",
      "encoder_layers.0.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.0.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.0.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.0.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.0.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.0.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.0.norm1.weight ==> 参数量：768\n",
      "encoder_layers.0.norm1.bias ==> 参数量：768\n",
      "encoder_layers.0.norm2.weight ==> 参数量：768\n",
      "encoder_layers.0.norm2.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.1.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.1.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.1.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.1.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.1.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.1.norm1.weight ==> 参数量：768\n",
      "encoder_layers.1.norm1.bias ==> 参数量：768\n",
      "encoder_layers.1.norm2.weight ==> 参数量：768\n",
      "encoder_layers.1.norm2.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.2.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.2.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.2.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.2.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.2.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.2.norm1.weight ==> 参数量：768\n",
      "encoder_layers.2.norm1.bias ==> 参数量：768\n",
      "encoder_layers.2.norm2.weight ==> 参数量：768\n",
      "encoder_layers.2.norm2.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.3.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.3.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.3.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.3.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.3.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.3.norm1.weight ==> 参数量：768\n",
      "encoder_layers.3.norm1.bias ==> 参数量：768\n",
      "encoder_layers.3.norm2.weight ==> 参数量：768\n",
      "encoder_layers.3.norm2.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.4.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.4.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.4.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.4.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.4.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.4.norm1.weight ==> 参数量：768\n",
      "encoder_layers.4.norm1.bias ==> 参数量：768\n",
      "encoder_layers.4.norm2.weight ==> 参数量：768\n",
      "encoder_layers.4.norm2.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.q_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.q_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.k_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.k_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.v_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.v_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.self_attn.out_linear.weight ==> 参数量：589824\n",
      "encoder_layers.5.self_attn.out_linear.bias ==> 参数量：768\n",
      "encoder_layers.5.ff.linear1.weight ==> 参数量：1572864\n",
      "encoder_layers.5.ff.linear1.bias ==> 参数量：2048\n",
      "encoder_layers.5.ff.linear2.weight ==> 参数量：1572864\n",
      "encoder_layers.5.ff.linear2.bias ==> 参数量：768\n",
      "encoder_layers.5.norm1.weight ==> 参数量：768\n",
      "encoder_layers.5.norm1.bias ==> 参数量：768\n",
      "encoder_layers.5.norm2.weight ==> 参数量：768\n",
      "encoder_layers.5.norm2.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.0.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.0.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.0.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.0.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.0.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.0.norm1.weight ==> 参数量：768\n",
      "decoder_layers.0.norm1.bias ==> 参数量：768\n",
      "decoder_layers.0.norm2.weight ==> 参数量：768\n",
      "decoder_layers.0.norm2.bias ==> 参数量：768\n",
      "decoder_layers.0.norm3.weight ==> 参数量：768\n",
      "decoder_layers.0.norm3.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.1.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.1.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.1.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.1.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.1.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.1.norm1.weight ==> 参数量：768\n",
      "decoder_layers.1.norm1.bias ==> 参数量：768\n",
      "decoder_layers.1.norm2.weight ==> 参数量：768\n",
      "decoder_layers.1.norm2.bias ==> 参数量：768\n",
      "decoder_layers.1.norm3.weight ==> 参数量：768\n",
      "decoder_layers.1.norm3.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.2.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.2.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.2.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.2.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.2.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.2.norm1.weight ==> 参数量：768\n",
      "decoder_layers.2.norm1.bias ==> 参数量：768\n",
      "decoder_layers.2.norm2.weight ==> 参数量：768\n",
      "decoder_layers.2.norm2.bias ==> 参数量：768\n",
      "decoder_layers.2.norm3.weight ==> 参数量：768\n",
      "decoder_layers.2.norm3.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.3.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.3.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.3.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.3.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.3.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.3.norm1.weight ==> 参数量：768\n",
      "decoder_layers.3.norm1.bias ==> 参数量：768\n",
      "decoder_layers.3.norm2.weight ==> 参数量：768\n",
      "decoder_layers.3.norm2.bias ==> 参数量：768\n",
      "decoder_layers.3.norm3.weight ==> 参数量：768\n",
      "decoder_layers.3.norm3.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.4.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.4.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.4.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.4.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.4.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.4.norm1.weight ==> 参数量：768\n",
      "decoder_layers.4.norm1.bias ==> 参数量：768\n",
      "decoder_layers.4.norm2.weight ==> 参数量：768\n",
      "decoder_layers.4.norm2.bias ==> 参数量：768\n",
      "decoder_layers.4.norm3.weight ==> 参数量：768\n",
      "decoder_layers.4.norm3.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.self_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.self_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.q_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.q_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.k_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.k_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.v_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.v_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.cross_attn.out_linear.weight ==> 参数量：589824\n",
      "decoder_layers.5.cross_attn.out_linear.bias ==> 参数量：768\n",
      "decoder_layers.5.ff.linear1.weight ==> 参数量：1572864\n",
      "decoder_layers.5.ff.linear1.bias ==> 参数量：2048\n",
      "decoder_layers.5.ff.linear2.weight ==> 参数量：1572864\n",
      "decoder_layers.5.ff.linear2.bias ==> 参数量：768\n",
      "decoder_layers.5.norm1.weight ==> 参数量：768\n",
      "decoder_layers.5.norm1.bias ==> 参数量：768\n",
      "decoder_layers.5.norm2.weight ==> 参数量：768\n",
      "decoder_layers.5.norm2.bias ==> 参数量：768\n",
      "decoder_layers.5.norm3.weight ==> 参数量：768\n",
      "decoder_layers.5.norm3.bias ==> 参数量：768\n",
      "output_layer.weight ==> 参数量：15360000\n",
      "output_layer.bias ==> 参数量：20000\n",
      "111091232\n"
     ]
    }
   ],
   "source": [
    "# 组合T5模型\n",
    "class T5Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = T5Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src_input, tgt_input, src_mask=None, tgt_mask=None): \n",
    "        src_emb = self.pos_encoding(self.embedding(src_input))\n",
    "        tgt_emb = self.pos_encoding(self.embedding(tgt_input))\n",
    "        \n",
    "        enc_output = src_emb \n",
    "        for layer in self.encoder_layers:\n",
    "            enc_output = layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return self.output_layer(dec_output)\n",
    "    \n",
    "t5 = T5Model(20000, 768, 8, 2048, 6, 0.1)\n",
    "\n",
    "sum = 0 \n",
    "\n",
    "for name,param in t5.named_parameters(): \n",
    "    print(f\"{name} ==> 参数量：{param.numel()}\")\n",
    "    sum += param.numel()\n",
    "    \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成词汇表\n",
    "# from generate_tokenizer import gen_tokenizer\n",
    "\n",
    "# # 传入语料库文件， 输出tokenizer的json文件\n",
    "# src_path = \"corpus.txt\"\n",
    "# tokenizer_path = \"translation.json\"\n",
    "# gen_tokenizer(src_path, tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD]\n",
      "特殊 token 映射： {'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}\n",
      "Pad token ID: 0\n",
      "BOS token ID: 2\n",
      "BOS token ID: 2\n",
      "EOS token ID: 3\n",
      "PAD token ID: 0\n",
      "EOS token ID: 3\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 加载 tokenizer，显式设置特殊 token\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"translation.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\"\n",
    ")\n",
    "\n",
    "# 打印特殊 token 和映射\n",
    "print(\"Pad token:\", tokenizer.pad_token)\n",
    "print(\"特殊 token 映射：\", tokenizer.special_tokens_map)\n",
    "print(\"Pad token ID:\", tokenizer.pad_token_id)\n",
    "print(\"BOS token ID:\", tokenizer.convert_tokens_to_ids(\"[BOS]\"))\n",
    "print(\"BOS token ID:\", tokenizer.convert_tokens_to_ids(\"[BOS]\"))\n",
    "print(\"EOS token ID:\", tokenizer.convert_tokens_to_ids(\"[EOS]\"))\n",
    "print(\"PAD token ID:\", tokenizer.convert_tokens_to_ids(\"[PAD]\"))\n",
    "print(\"EOS token ID:\", tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: [2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3]\n",
      "分词结果： ['Hello', ',', 'world', '!', '你好', '，', '世界', '！']\n",
      "Encoded 输出: {'input_ids': tensor([[    2, 14860,    18, 10298,     7, 13791,  9209,  9863,  9198,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Input IDs: tensor([[    2, 14860,    18, 10298,     7, 13791,  9209,  9863,  9198,     3]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "带特殊 token 的分词结果： ['[BOS]', 'Hello', ',', 'world', '!', '你好', '，', '世界', '！', '[EOS]']\n"
     ]
    }
   ],
   "source": [
    "# 输入文本\n",
    "text = \"Hello, world! 你好，世界！\"\n",
    "\n",
    "# 转换为 token ID\n",
    "input_ids = tokenizer.encode(text)\n",
    "print(f\"Token ID: {input_ids}\")\n",
    "\n",
    "# 查看分词后的 token\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"分词结果：\", tokens)\n",
    "\n",
    "encoded = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "print(\"Encoded 输出:\", encoded)\n",
    "\n",
    "# 提取 input_ids 和 attention_mask\n",
    "input_ids = encoded[\"input_ids\"]\n",
    "attention_mask = encoded[\"attention_mask\"]\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "\n",
    "# 转换为 token 查看\n",
    "tokens_with_special = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(\"带特殊 token 的分词结果：\", tokens_with_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID:[2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3]\n",
      "分词结果： ['Hello', ',', 'world', '!', '你好', '，', '世界', '！']\n",
      "Encoded 输出: {'input_ids': [2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Input IDs: [2, 14860, 18, 10298, 7, 13791, 9209, 9863, 9198, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 输入文本\n",
    "text = \"Hello, world! 你好，世界！\"\n",
    "\n",
    "# 转换为token ID \n",
    "input_ids = tokenizer.encode(text)\n",
    "print(f\"Token ID:{input_ids}\")\n",
    "\n",
    "# 查看分词后的token\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"分词结果：\", tokens)\n",
    "\n",
    "encoded = tokenizer(\n",
    "    text,                   \n",
    "    # return_tensors=\"pt\",        # 返回 PyTorch 张量（\"tf\" 表示 TensorFlow，None 表示普通列表）\n",
    "    padding=\"max_length\",               # 自动填充（如果处理批量文本）\n",
    "    truncation=True,            # 自动截断（如果超过最大长度）\n",
    "    max_length=512              # 最大序列长度\n",
    ")\n",
    "\n",
    "print(\"Encoded 输出:\", encoded)\n",
    "\n",
    "# 提取 input_ids 和 attention_mask\n",
    "input_ids = encoded[\"input_ids\"]\n",
    "attention_mask = encoded[\"attention_mask\"]\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset): \n",
    "    def __init__(self, dataset, tokenizer, max_length = 10):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = [self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token)]\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        translation = self.dataset[index]['translation']\n",
    "        input = self.tokenizer(\n",
    "            translation[\"en\"],                   \n",
    "            # return_tensors=\"pt\",        # 返回 PyTorch 张量（\"tf\" 表示 TensorFlow，None 表示普通列表）\n",
    "            padding=\"max_length\",               # 自动填充（如果处理批量文本）\n",
    "            truncation=True,            # 自动截断（如果超过最大长度）\n",
    "            max_length=self.max_length              # 最大序列长度\n",
    "        )\n",
    "        output = self.tokenizer(\n",
    "            translation[\"zh\"],                   \n",
    "            # return_tensors=\"pt\",        # 返回 PyTorch 张量（\"tf\" 表示 TensorFlow，None 表示普通列表）\n",
    "            padding=\"max_length\",               # 自动填充（如果处理批量文本）\n",
    "            truncation=True,            # 自动截断（如果超过最大长度）\n",
    "            max_length=self.max_length              # 最大序列长度\n",
    "        )\n",
    "\n",
    "        # 提取 input_ids（去掉 batch 维度）\n",
    "        src_input = input[\"input_ids\"] # [seq_len]\n",
    "        src_attention_mask = input[\"attention_mask\"]\n",
    "        tgt_input = output[\"input_ids\"] # [seq_len] \n",
    "        tgt_output = tgt_input[1:] + self.pad_token_id # 去掉第一个 token（通常是 <BOS>）\n",
    "        \n",
    "        return [src_input, src_attention_mask, tgt_input, tgt_output]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_input, src_attention_mask, tgt_input, tgt_output = zip(*batch)\n",
    "    return (torch.tensor(src_input, dtype=torch.long)\n",
    "            , torch.tensor(src_attention_mask, dtype=torch.long).unsqueeze(1).unsqueeze(2)\n",
    "            , torch.tensor(tgt_input, dtype=torch.long)\n",
    "            , torch.tensor(tgt_output, dtype=torch.long))\n",
    "        \n",
    "train_dataset = TranslationDataset(dataset=dataset[\"train\"], tokenizer=tokenizer, max_length=15)\n",
    "train_loader = DataLoader(train_dataset, batch_size=160, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "\n",
    "model = T5Model(vocab_size, d_model, num_heads, d_ff, num_layers, dropout)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76938496\n"
     ]
    }
   ],
   "source": [
    "param_num = [para.numel() for para in model.parameters()]\n",
    "sum = 0 \n",
    "for n in param_num:\n",
    "    sum += n\n",
    "    \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 1, 1, 10])\n",
      "torch.Size([96, 1, 1, 1, 1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 11946,  9209,  9609,  1270,  6746,  7460,  3444, 26296,     3],\n",
       "        [    2, 10381,  2688, 10604, 10406, 12602,  9209,  2256, 25404,     3],\n",
       "        [    2,    45, 10978,    18,  5303,  2166,  7434, 13365,     3,     0],\n",
       "        [    2, 16049, 13932,     3,     0,     0,     0,     0,     0,     0],\n",
       "        [    2,    23,    20,  7224,  6999, 20563,  9812, 10115, 26918,     3],\n",
       "        [    2,  1952,     3,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    2, 24134,  9209, 19631, 10071, 22235, 12865,  9209,  2671,     3],\n",
       "        [    2, 13939,     3,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    2, 17927,  1813, 16046, 11085,  7628,  2372, 10931,  9506,     3],\n",
       "        [    2,  1111,  9733, 11709,  1000, 19505,   799,     3,     0,     0],\n",
       "        [    2, 12499, 16681, 13915,  9863,     3,     0,     0,     0,     0],\n",
       "        [    2, 12291, 11052,  9621,     3,     0,     0,     0,     0,     0],\n",
       "        [    2,   813,    24,    20, 10017,  7431, 12024,  9209, 11171,     3],\n",
       "        [    2, 21027,  9209, 12553,  7059,  2949,  1300,  1169,  3734,     3],\n",
       "        [    2, 10465,   805, 12738,  2763,  1582,  4038,   806,  4935,     3],\n",
       "        [    2,  2095,  4626,  2329,  1945,  4197,  7162,    37,     3,     0],\n",
       "        [    2, 18621, 10205,  9627, 31545,  9655,  9647,  9209,  1389,     3],\n",
       "        [    2,  3034,  3966,  7507, 25537,  2826,  6871,  1180,  9209,     3],\n",
       "        [    2, 10683, 16163, 17953, 30636, 18438, 12365,  9209,  9827,     3],\n",
       "        [    2, 15231,  9965, 10222,  9209,  1095,  1666,  9436, 11646,     3],\n",
       "        [    2, 13614,  9802,  3556,  3034,  4756, 11722,  9973, 10328,     3],\n",
       "        [    2, 26287,  4009, 30183, 10506,  1064,     3,     0,     0,     0],\n",
       "        [    2, 16867, 12155,  9209, 20298, 14977, 10102, 13226,  1004,     3],\n",
       "        [    2, 21544,  3123,  8229,  2509, 30653, 10623,  1637,   995,     3],\n",
       "        [    2,    14,    83,    15, 12696, 10848, 15692, 18022, 26354,     3],\n",
       "        [    2,  2279,  1003,  2502,  8475, 28394,  2204,     3,     0,     0],\n",
       "        [    2, 12130,  5108,  9856,  9209, 12130,  9198,     3,     0,     0],\n",
       "        [    2,    19,  1949,    18, 13589,     3,     0,     0,     0,     0],\n",
       "        [    2, 18358, 10056, 11367,  1064, 18171, 16992,  1004, 29712,     3],\n",
       "        [    2,  9762,  2168,  8125, 29887,  5343,  4584, 22356, 11814,     3],\n",
       "        [    2,  2402, 10838, 18713, 17020, 18824, 11077,  1004, 12574,     3],\n",
       "        [    2, 10349, 14297,  9209,  5472, 27634,  8098,   799,     3,     0],\n",
       "        [    2,  1687, 16649, 21096,  9843,     3,     0,     0,     0,     0],\n",
       "        [    2,  9556,   639, 10083,  1069, 11669, 26977, 12074,  9223,     3],\n",
       "        [    2,  9609, 11018,  9882, 16252,    19, 14663, 14621,  9439,     3],\n",
       "        [    2, 15272,     3,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    2, 23118,  7047, 13849,    20,    31,    11, 11976,  2387,     3],\n",
       "        [    2, 12002,    20, 14351, 31858, 14122,  9760,  9640, 11896,     3],\n",
       "        [    2,    14,  1000,    15,  9607, 10083,   799,     3,     0,     0],\n",
       "        [    2,  1106,  3887, 13890,  9209, 14921,  1531,  2046, 20901,     3],\n",
       "        [    2, 10729,  9209,  2790, 19421, 24743,  9209, 17839,  9526,     3],\n",
       "        [    2, 15744,  2670,  9716, 14383, 17039,  2060, 12142,   639,     3],\n",
       "        [    2, 27283,  9209,  3604,  1729,  1111, 22100,  9209, 16094,     3],\n",
       "        [    2,   790,    47, 14019,    47,  9847,  9298, 13809,  9499,     3],\n",
       "        [    2,    97, 13874,  1606,  3420,  1129,  2407, 14680, 21658,     3],\n",
       "        [    2,  5472, 10956,    21, 26970,  9881,     3,     0,     0,     0],\n",
       "        [    2,  9210, 11933,  9209, 28809, 23147,     3,     0,     0,     0],\n",
       "        [    2,  9526, 26575,  2060, 15031, 29074, 22758, 10984, 24877,     3],\n",
       "        [    2, 15715,    20, 10913,  9209,  2475,  1472,  9980,    25,     3],\n",
       "        [    2, 23120, 13664, 14211,  9228,     3,     0,     0,     0,     0],\n",
       "        [    2,  4023,  4323,  6294,   799,     3,     0,     0,     0,     0],\n",
       "        [    2,  9483, 16187,  4977,  7431, 14363, 11061, 16991, 15271,     3],\n",
       "        [    2, 12472, 27249,     7,     3,     0,     0,     0,     0,     0],\n",
       "        [    2,  3560, 10021,  9852, 18898, 16748, 23058,     3,     0,     0],\n",
       "        [    2, 14323, 10211,     3,     0,     0,     0,     0,     0,     0],\n",
       "        [    2,  3440,  5785,  5710,  7564,  3558,  3086,  6942, 28393,     3],\n",
       "        [    2, 22506, 14341, 23171,  9684, 25944, 13216,  9209, 11504,     3],\n",
       "        [    2,  1111, 15680,  3555,  5717,  5569,  1002,    56,  9306,     3],\n",
       "        [    2, 22275,  1454, 28901,     3,     0,     0,     0,     0,     0],\n",
       "        [    2,  9961,    20, 14104, 31856,  9209, 14104, 31853, 11061,     3],\n",
       "        [    2, 11335, 10586, 16134, 24348,  7626, 18612, 12153, 14276,     3],\n",
       "        [    2, 14961, 11565, 13865, 10725,  9526,  3082,  3755,  4605,     3],\n",
       "        [    2,    19, 10328,  3034,     3,     0,     0,     0,     0,     0],\n",
       "        [    2, 13092,  2475,  4249,  9209,  5004,  2377, 14481,  4935,     3],\n",
       "        [    2,  9483,  3491, 12631,  9209, 10040, 16797,  9473, 22282,     3],\n",
       "        [    2,  1180, 12728,  1142, 30450,  9209,  8003, 12319, 18021,     3],\n",
       "        [    2, 30983, 11290,  2472, 13477, 17835, 11853,  4935, 31551,     3],\n",
       "        [    2, 28971,  3060, 31464, 11301, 10178, 18345,  1397, 28728,     3],\n",
       "        [    2,    46,    20,  9607, 12712, 18378,    19, 15003,  9699,     3],\n",
       "        [    2, 10162, 11345, 16970, 12501, 20850, 12780,  1783, 17814,     3],\n",
       "        [    2, 13138, 26820, 11741, 15836,  1690,  2671,  2378, 11651,     3],\n",
       "        [    2,  9553, 13717, 10271, 10099, 13551,    20,     3,     0,     0],\n",
       "        [    2,  1746,  9466,  1003,    20,    20,     3,     0,     0,     0],\n",
       "        [    2,    16, 15921,  2800,  6134,  3091,  3927,  2437,  3927,     3],\n",
       "        [    2,  5833,  8007, 30139,  6353,  1836,  9209, 12058,  1783,     3],\n",
       "        [    2, 12623, 22445,   799,     3,     0,     0,     0,     0,     0],\n",
       "        [    2,  9528,    20, 10733, 11849, 13018, 20733,  7061, 14706,     3],\n",
       "        [    2,    34, 13457, 13508, 13120,    20, 10936,    21, 10053,     3],\n",
       "        [    2, 10184,  5979,  7237,  6021,  7067,  2434, 31080,  9198,     3],\n",
       "        [    2,    25,    20, 20238,  9961,    19, 11638,    30,     3,     0],\n",
       "        [    2, 10021,  5105,  4455,  4009,  4764,  7499,  1390,  9209,     3],\n",
       "        [    2, 10264, 16392,  9439, 10512,  1007,  3691,  5855,  2060,     3],\n",
       "        [    2, 14071,    20, 10505, 12180, 22668,  9690, 12544,  1669,     3],\n",
       "        [    2, 11946,  2800,  7047,  7047,  8456,  2234,  2884,  3886,     3],\n",
       "        [    2,  9922,    20,  1660, 10838, 12551,  9968,  1707, 11093,     3],\n",
       "        [    2, 13598, 17090,    23,    20,    24,  9223, 19234, 10671,     3],\n",
       "        [    2, 11340, 19631, 10404,  9723,  9627,  9556, 17234, 19099,     3],\n",
       "        [    2,  3034,  5205,  3560,  3162,  2487,  4935,  7132,  1127,     3],\n",
       "        [    2, 10172, 31168, 31512, 28748,  2196,     3,     0,     0,     0],\n",
       "        [    2,  9205,    16, 11234, 10724,  9206,     3,     0,     0,     0],\n",
       "        [    2,  9885,    20,  9456, 12533, 22549, 22564, 12645,    21,     3],\n",
       "        [    2, 21649,  1729, 10007,  6302,  9556, 25249,  5336,  3755,     3],\n",
       "        [    2, 17147,  1332, 25936,  7175,  3350,  2645,  1001,  1025,     3],\n",
       "        [    2, 24392, 15451, 11852,  6977,  1180,  7242,  5694,  6456,     3],\n",
       "        [    2,  9690,    20,    58,    71,  9409,  9856,  9205, 12313,     3],\n",
       "        [    2,  5865, 28093,  7408,  6981,  9209, 21150,  4935,  7502,     3]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(next(iter(train_loader))[1].shape)\n",
    "aa = next(iter(train_loader))[1].unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "print(aa.shape)\n",
    "\n",
    "\n",
    "tgt_test = next(iter(train_loader))[2]\n",
    "tgt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_mask(tgt, pad_idx): \n",
    "    tgt_seq_len = tgt.size(1)\n",
    "    tgt_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len))).bool().to(tgt.device)\n",
    "    tgt_mask = tgt_mask & (tgt != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return tgt_mask \n",
    "\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "print(pad_token_id)\n",
    "\n",
    "test_size = tgt_test.shape[1]\n",
    "\n",
    "tgt_mask = torch.tril(torch.ones((test_size, test_size))).bool()\n",
    "tgt_mask = tgt_mask & (tgt_test != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "print(test_size)\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [17:00<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5, Loss:5.421014827575684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [16:50<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 5, Loss:4.214462287063599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [17:15<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 5, Loss:3.7815352058410645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [17:03<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 5, Loss:3.521689265899658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [16:47<00:00,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 / 5, Loss:3.3347109633636474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "num_epochs = 5 \n",
    "for epoch in range(num_epochs): \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (src_input, src_attention_mask, tgt_input, tgt_output) in tqdm(train_loader):\n",
    "        src_input = src_input.to(device)\n",
    "        tgt_input = tgt_input.to(device)\n",
    "        tgt_output = tgt_output.to(device)\n",
    "        src_attention_mask = src_attention_mask.to(device)\n",
    "        \n",
    "        tgt_mask = create_mask(tgt_input, pad_token_id)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src_input, tgt_input, src_attention_mask, tgt_mask)\n",
    "        \n",
    "        # test测试是否正确配置项目\n",
    "        # print(f\"tgt_output.shape:{tgt_output.shape}\")      # tgt_output.shape:torch.Size([2, 10])\n",
    "        # print(f\"logits.shape:{logits.shape}\")              # logits.shape:torch.Size([2, 10, 32000])\n",
    "        # break \n",
    "    \n",
    "        loss = loss_fn(logits.view(-1, vocab_size), tgt_output.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs}, Loss:{total_loss / len(train_loader)}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_text, max_length=10000): \n",
    "    model.eval()\n",
    "    input = tokenizer(text=src_text, return_tensors=\"pt\")\n",
    "    src_input = input['input_ids'].to(device)\n",
    "    src_mask_attention = input['attention_mask'].to(device)\n",
    "    print(src_input)\n",
    "    print(src_mask_attention)\n",
    "\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    tgt_ids = [bos_id]\n",
    "    for _ in range(max_length):\n",
    "        tgt_input = torch.tensor([tgt_ids], dtype=torch.long).to(device)\n",
    "        tgt_mask = torch.tril(torch.ones((len(tgt_ids), len(tgt_ids)))).bool().to(device)\n",
    "        tgt_mask = tgt_mask & (tgt_input != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(src_input, tgt_input, src_mask_attention, tgt_mask)\n",
    "            next_token = logits[0, -1].argmax().item()\n",
    "            \n",
    "        if next_token == eos_id:\n",
    "            break \n",
    "        \n",
    "        tgt_ids.append(next_token)\n",
    "        \n",
    "\n",
    "    return ''.join(tokenizer.convert_ids_to_tokens(tgt_ids)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,    42, 12000,    89, 10841, 10850, 12409,  9312, 16620,    20,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "翻译结果：Dreamsgivelifeand方向.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "src_text = \"Dreams give life purpose and direction.\"\n",
    "\n",
    "translated = translate(model, src_text)\n",
    "\n",
    "print(f\"翻译结果：{translated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
